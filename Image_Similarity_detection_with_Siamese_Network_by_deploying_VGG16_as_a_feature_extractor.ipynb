{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bty2_hA8jzil"
      },
      "source": [
        "import os\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "from skimage.color import rgb2gray\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7sueyyApJuA",
        "outputId": "80223172-6095-4c51-b5a4-df4857c8b760"
      },
      "source": [
        "#mounting google drive to load data \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "mra2pWNgjziu",
        "outputId": "8da7ff7c-8dbb-4f2f-f27f-151b03ae7aed"
      },
      "source": [
        "#Loading the images and storing into a list by cropping and \n",
        "#resizing to (80,50) to maintain the aspect ratio of original image data\n",
        "import os\n",
        "from skimage.transform import resize\n",
        "from skimage.util import crop\n",
        "#the image path from google drive\n",
        "path =\"/content/drive/My Drive/Synthetic_Leopard_Circle\"\n",
        "imageDirectory=os.listdir(path) \n",
        "\n",
        "depth=[]\n",
        "allImg_list=[]\n",
        "for img in imageDirectory:\n",
        "  if \"col\" in img:\n",
        "    image=(io.imread(os.path.join(path,img))) \n",
        "    imgCr=crop(image, ((50, 100), (50, 100), (0,0)), copy=False)\n",
        "    img_resized = resize(imgCr, (80, 50))\n",
        "    allImg_list.append(img_resized) \n",
        "  else:\n",
        "      depth.append(\"depth File not needed\")\n",
        "\n",
        "print(plt.imshow(allImg_list[0]))\n",
        "print(\"Image size:\", allImg_list[0].shape)\n",
        "\n",
        "#Converting img list into array\n",
        "allImg_list=np.array(allImg_list)\n",
        "print(allImg_list.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AxesImage(54,36;334.8x217.44)\n",
            "Image size: (80, 50, 3)\n",
            "120\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK5qHTZj9Gkq"
      },
      "source": [
        "###splitting the images into train,valid and test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLillaK4lzm9",
        "outputId": "a20fbdee-97f2-4552-a465-3ed0ea32d1d9"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "splitTrain=list(range(90))\n",
        "splitValid=list(range(90, 100))\n",
        "splitTest=list(range(100,120))\n",
        "print(\"Size of Train split:\",len(splitTrain))\n",
        "print(\"Size of Validation split:\",len(splitValid))\n",
        "print(\"Size of Test split:\",len(splitTest))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Train split: 90\n",
            "Size of Validation split: 10\n",
            "Size of Test split: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY5RO4B_9NpF"
      },
      "source": [
        "### Pairing the images into Train, valid ans Test pairs along with their labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE5LwWTGSz0d"
      },
      "source": [
        "image_pairsTrain=[]\n",
        "image_pairsValid=[]\n",
        "image_pairsTest=[]\n",
        "angle_DiffTrain=[]\n",
        "angle_DiffValid=[]\n",
        "angle_DiffTest=[]\n",
        "for i in range (0,allImg_list.shape[0]):\n",
        "    \n",
        "    for j in range(-15,16):\n",
        "      image=allImg_list[i]\n",
        "      nextImage=allImg_list[(i+j)%allImg_list.shape[0]]\n",
        "      img=np.concatenate((image, nextImage),axis=2)\n",
        "      angularDif=j*3\n",
        "      if i in splitTrain:\n",
        "        image_pairsTrain.append(img)\n",
        "        angle_DiffTrain.append(angularDif)\n",
        "      \n",
        "      elif i in splitValid:\n",
        "        image_pairsValid.append(img)\n",
        "        angle_DiffValid.append(angularDif)\n",
        "      \n",
        "      else:\n",
        "        image_pairsTest.append(img)\n",
        "        angle_DiffTest.append(angularDif)\n",
        "\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj5ymzOJ9iUr"
      },
      "source": [
        "### Encoding the labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me9_ngUqIzzw",
        "outputId": "cafa766d-bf41-4948-c075-8d547e59742f"
      },
      "source": [
        "# import LabelEncoder \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "# Instatniate LabelEncoder\n",
        "le = LabelEncoder()\n",
        "# LabelEncode the labels\n",
        "yTrain= le.fit_transform(angle_DiffTrain)\n",
        "yValid= le.fit_transform(angle_DiffValid)\n",
        "yTest= le.fit_transform(angle_DiffTest)\n",
        "\n",
        "#converting labels to categorical\n",
        "yTrain = to_categorical(yTrain)\n",
        "yValid= to_categorical(yValid)\n",
        "yTest = to_categorical(yTest) \n",
        "print(\"ytrain shape:\", yTrain.shape)\n",
        "print(\"Train Labels:\",len(yTrain))\n",
        "print(\"Validation labels:\", len(yValid))\n",
        "print(\"Test Labels:\", len(yTest))\n",
        "print(\"class labels in yTrain:\", np.unique(angle_DiffTrain))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ytrain shape: (2790, 31)\n",
            "Train Labels: 2790\n",
            "Validation labels: 310\n",
            "Test Labels: 620\n",
            "class labels in yTrain: [-45 -42 -39 -36 -33 -30 -27 -24 -21 -18 -15 -12  -9  -6  -3   0   3   6\n",
            "   9  12  15  18  21  24  27  30  33  36  39  42  45]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6qvtQVajzjZ",
        "outputId": "134e2999-4c47-4655-fc28-0b83a77a67cf"
      },
      "source": [
        "#converting Train data into numpy array\n",
        "xTrain=np.array(image_pairsTrain)\n",
        "print(\"Shape of image pairs array:\", xTrain.shape)\n",
        "xValid=np.array(image_pairsValid)\n",
        "xTest=np.array(image_pairsTest)\n",
        "print(\"Numebr of train pairs for train split with start position 90 :\", len(xTrain))\n",
        "print(\"Number of Valiadtion pairs for valid split with start position 10 :\", len(xValid))\n",
        "print(\"Number of Test pairs for test split with start position 20 :\", len(xTest))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of image pairs array: (2790, 80, 50, 6)\n",
            "Numebr of train pairs for train split with start position 90 : 2790\n",
            "Number of Valiadtion pairs for valid split with start position 10 : 310\n",
            "Number of Test pairs for test split with start position 20 : 620\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nGJVbt2xt0r",
        "outputId": "716d8619-e1e8-47c6-b78e-ba6c4a33e896"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "#define model\n",
        "my_model3 =Sequential()\n",
        "\n",
        "# 1st block \n",
        "my_model3.add(Conv2D(32, (3, 3), activation='relu', padding='same', \n",
        "                    input_shape=(80, 50, 6)))\n",
        "my_model3.add(BatchNormalization())\n",
        "my_model3.add(MaxPooling2D((2, 2), padding='same'))\n",
        "\n",
        "# Second block\n",
        "my_model3.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "my_model3.add(BatchNormalization())\n",
        "my_model3.add(MaxPooling2D((2, 2), padding='same'))\n",
        "\n",
        "#3rd block \n",
        "my_model3.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "my_model3.add(BatchNormalization())\n",
        "my_model3.add(MaxPooling2D((2, 2), padding='same'))\n",
        "\n",
        "# this converts our 3D feature maps to 1D feature vectors\n",
        "my_model3.add(Flatten())\n",
        "# fully connected layer\n",
        "my_model3.add(Dense(512, activation='relu'))\n",
        "my_model3.add(BatchNormalization())\n",
        "#my_model3.add(Dropout(0.5))\n",
        "# make predictions\n",
        "my_model3.add(Dense(31, activation='softmax'))\n",
        "\n",
        "# Show a summary of the model. Check the number of trainable parameters\n",
        "my_model3.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 80, 50, 32)        1760      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 80, 50, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 40, 25, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 40, 25, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 40, 25, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 20, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 20, 13, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 20, 13, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 10, 7, 128)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 8960)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               4588032   \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 31)                15903     \n",
            "=================================================================\n",
            "Total params: 4,700,991\n",
            "Trainable params: 4,699,519\n",
            "Non-trainable params: 1,472\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxBX_wmARAAY"
      },
      "source": [
        "#Creating dataGenerator\n",
        "import tensorflow as tf\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJmkt4ztBw5i"
      },
      "source": [
        "# use early stopping to optimally terminate training through callbacks\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=155)\n",
        "\n",
        "# save best model automatically\n",
        "mc= ModelCheckpoint('yourdirectory/CNN2D_model.h5', monitor='val_loss', \n",
        "                    mode='min', verbose=1, save_best_only=True)\n",
        "cb_list=[es,mc]\n",
        "\n",
        "# compile model \n",
        "my_model3.compile(optimizer='adam', loss='categorical_crossentropy', \n",
        "                 metrics=['accuracy'])\n",
        "#categorical_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUxItEtRDOvF",
        "outputId": "2cd193c6-20be-49f8-b082-9362b319de77"
      },
      "source": [
        "history = my_model3.fit_generator(\n",
        "        datagen.flow(xTrain, yTrain, batch_size=30),\n",
        "        \n",
        "        epochs=155,\n",
        "        validation_data=datagen.flow(xValid, yValid, batch_size=30),\n",
        "        callbacks=cb_list)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/155\n",
            "11/93 [==>...........................] - ETA: 0s - loss: 0.0092 - accuracy: 1.0000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py:136: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (2790, 80, 50, 6) (6 channels).\n",
            "  str(self.x.shape[channels_axis]) + ' channels).')\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py:136: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (310, 80, 50, 6) (6 channels).\n",
            "  str(self.x.shape[channels_axis]) + ' channels).')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0111 - accuracy: 0.9981\n",
            "Epoch 00001: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0118 - accuracy: 0.9978 - val_loss: 1618.3325 - val_accuracy: 0.0355\n",
            "Epoch 2/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0100 - accuracy: 0.9981\n",
            "Epoch 00002: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0101 - accuracy: 0.9982 - val_loss: 6971.3477 - val_accuracy: 0.0419\n",
            "Epoch 3/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0065 - accuracy: 0.9996\n",
            "Epoch 00003: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0065 - accuracy: 0.9996 - val_loss: 1381.5142 - val_accuracy: 0.0355\n",
            "Epoch 4/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0077 - accuracy: 0.9978\n",
            "Epoch 00004: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0077 - accuracy: 0.9978 - val_loss: 3327.8547 - val_accuracy: 0.0387\n",
            "Epoch 5/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0089 - accuracy: 0.9989\n",
            "Epoch 00005: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0087 - accuracy: 0.9989 - val_loss: 14274.7949 - val_accuracy: 0.0419\n",
            "Epoch 6/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0053 - accuracy: 0.9996\n",
            "Epoch 00006: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0053 - accuracy: 0.9996 - val_loss: 6515.2051 - val_accuracy: 0.0323\n",
            "Epoch 7/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0067 - accuracy: 0.9989\n",
            "Epoch 00007: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0066 - accuracy: 0.9989 - val_loss: 21394.6719 - val_accuracy: 0.0484\n",
            "Epoch 8/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0059 - accuracy: 0.9989\n",
            "Epoch 00008: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0058 - accuracy: 0.9989 - val_loss: 8955.9424 - val_accuracy: 0.0355\n",
            "Epoch 9/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0087 - accuracy: 0.9985\n",
            "Epoch 00009: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0086 - accuracy: 0.9986 - val_loss: 4328.4121 - val_accuracy: 0.0419\n",
            "Epoch 10/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0050 - accuracy: 0.9993\n",
            "Epoch 00010: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0050 - accuracy: 0.9993 - val_loss: 11386.5107 - val_accuracy: 0.0290\n",
            "Epoch 11/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0095 - accuracy: 0.9993\n",
            "Epoch 00011: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0093 - accuracy: 0.9993 - val_loss: 8864.9355 - val_accuracy: 0.0355\n",
            "Epoch 12/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0145 - accuracy: 0.9978\n",
            "Epoch 00012: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0140 - accuracy: 0.9978 - val_loss: 5656.6294 - val_accuracy: 0.0548\n",
            "Epoch 13/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0137 - accuracy: 0.9966\n",
            "Epoch 00013: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0132 - accuracy: 0.9968 - val_loss: 26018.4785 - val_accuracy: 0.0323\n",
            "Epoch 14/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0388 - accuracy: 0.9927\n",
            "Epoch 00014: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0381 - accuracy: 0.9928 - val_loss: 35996.3672 - val_accuracy: 0.0323\n",
            "Epoch 15/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0374 - accuracy: 0.9913\n",
            "Epoch 00015: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0371 - accuracy: 0.9914 - val_loss: 3635.0422 - val_accuracy: 0.0323\n",
            "Epoch 16/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0784 - accuracy: 0.9818\n",
            "Epoch 00016: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0821 - accuracy: 0.9810 - val_loss: 22821.8535 - val_accuracy: 0.0258\n",
            "Epoch 17/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.1089 - accuracy: 0.9693\n",
            "Epoch 00017: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1148 - accuracy: 0.9674 - val_loss: 68975.8203 - val_accuracy: 0.0323\n",
            "Epoch 18/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.1584 - accuracy: 0.9533\n",
            "Epoch 00018: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1612 - accuracy: 0.9523 - val_loss: 11551.8213 - val_accuracy: 0.0323\n",
            "Epoch 19/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.1513 - accuracy: 0.9544\n",
            "Epoch 00019: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1504 - accuracy: 0.9556 - val_loss: 70054.1875 - val_accuracy: 0.0323\n",
            "Epoch 20/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.1147 - accuracy: 0.9655\n",
            "Epoch 00020: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1154 - accuracy: 0.9649 - val_loss: 26125.1074 - val_accuracy: 0.0323\n",
            "Epoch 21/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0673 - accuracy: 0.9833\n",
            "Epoch 00021: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0665 - accuracy: 0.9832 - val_loss: 6486.4170 - val_accuracy: 0.0323\n",
            "Epoch 22/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9925\n",
            "Epoch 00022: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0260 - accuracy: 0.9925 - val_loss: 1732.9978 - val_accuracy: 0.0355\n",
            "Epoch 23/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0192 - accuracy: 0.9966\n",
            "Epoch 00023: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0187 - accuracy: 0.9968 - val_loss: 36207.3320 - val_accuracy: 0.0323\n",
            "Epoch 24/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0138 - accuracy: 0.9971\n",
            "Epoch 00024: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0137 - accuracy: 0.9971 - val_loss: 8712.9736 - val_accuracy: 0.0323\n",
            "Epoch 25/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9986\n",
            "Epoch 00025: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0108 - accuracy: 0.9986 - val_loss: 1321.8556 - val_accuracy: 0.0419\n",
            "Epoch 26/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0293 - accuracy: 0.9942\n",
            "Epoch 00026: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0290 - accuracy: 0.9943 - val_loss: 26335.2480 - val_accuracy: 0.0355\n",
            "Epoch 27/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0268 - accuracy: 0.9948\n",
            "Epoch 00027: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0265 - accuracy: 0.9946 - val_loss: 23098.8359 - val_accuracy: 0.0419\n",
            "Epoch 28/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0278 - accuracy: 0.9933\n",
            "Epoch 00028: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0308 - accuracy: 0.9932 - val_loss: 27.9049 - val_accuracy: 0.0355\n",
            "Epoch 29/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0363 - accuracy: 0.9939\n",
            "Epoch 00029: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0392 - accuracy: 0.9928 - val_loss: 49128.9961 - val_accuracy: 0.0323\n",
            "Epoch 30/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9932\n",
            "Epoch 00030: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0264 - accuracy: 0.9932 - val_loss: 40193.1992 - val_accuracy: 0.0258\n",
            "Epoch 31/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0273 - accuracy: 0.9936\n",
            "Epoch 00031: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0296 - accuracy: 0.9932 - val_loss: 44497.0273 - val_accuracy: 0.0290\n",
            "Epoch 32/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0293 - accuracy: 0.9928\n",
            "Epoch 00032: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0296 - accuracy: 0.9928 - val_loss: 22598.4746 - val_accuracy: 0.0323\n",
            "Epoch 33/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0584 - accuracy: 0.9841\n",
            "Epoch 00033: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0601 - accuracy: 0.9839 - val_loss: 34769.7305 - val_accuracy: 0.0323\n",
            "Epoch 34/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.1057 - accuracy: 0.9746\n",
            "Epoch 00034: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1066 - accuracy: 0.9735 - val_loss: 108198.2812 - val_accuracy: 0.0323\n",
            "Epoch 35/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.1222 - accuracy: 0.9686\n",
            "Epoch 00035: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1278 - accuracy: 0.9677 - val_loss: 4559.7466 - val_accuracy: 0.0323\n",
            "Epoch 36/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.1510 - accuracy: 0.9599\n",
            "Epoch 00036: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1489 - accuracy: 0.9606 - val_loss: 1414.7458 - val_accuracy: 0.0323\n",
            "Epoch 37/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.1037 - accuracy: 0.9719\n",
            "Epoch 00037: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1025 - accuracy: 0.9717 - val_loss: 60022.3242 - val_accuracy: 0.0355\n",
            "Epoch 38/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0573 - accuracy: 0.9850\n",
            "Epoch 00038: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0555 - accuracy: 0.9857 - val_loss: 88174.6953 - val_accuracy: 0.0323\n",
            "Epoch 39/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0459 - accuracy: 0.9907\n",
            "Epoch 00039: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0471 - accuracy: 0.9900 - val_loss: 22100.0332 - val_accuracy: 0.0323\n",
            "Epoch 40/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0411 - accuracy: 0.9907\n",
            "Epoch 00040: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0405 - accuracy: 0.9907 - val_loss: 17403.1484 - val_accuracy: 0.0323\n",
            "Epoch 41/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0215 - accuracy: 0.9956\n",
            "Epoch 00041: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0212 - accuracy: 0.9957 - val_loss: 3327.0222 - val_accuracy: 0.0323\n",
            "Epoch 42/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0136 - accuracy: 0.9966\n",
            "Epoch 00042: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0140 - accuracy: 0.9964 - val_loss: 5180.5737 - val_accuracy: 0.0323\n",
            "Epoch 43/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0215 - accuracy: 0.9963\n",
            "Epoch 00043: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0209 - accuracy: 0.9964 - val_loss: 13347.0713 - val_accuracy: 0.0323\n",
            "Epoch 44/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0076 - accuracy: 0.9985\n",
            "Epoch 00044: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0075 - accuracy: 0.9986 - val_loss: 12583.8545 - val_accuracy: 0.0323\n",
            "Epoch 45/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0111 - accuracy: 0.9981\n",
            "Epoch 00045: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0110 - accuracy: 0.9978 - val_loss: 18680.3477 - val_accuracy: 0.0323\n",
            "Epoch 46/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0206 - accuracy: 0.9959\n",
            "Epoch 00046: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0202 - accuracy: 0.9961 - val_loss: 46044.3789 - val_accuracy: 0.0323\n",
            "Epoch 47/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0224 - accuracy: 0.9938\n",
            "Epoch 00047: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0220 - accuracy: 0.9939 - val_loss: 71989.5625 - val_accuracy: 0.0323\n",
            "Epoch 48/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0334 - accuracy: 0.9921\n",
            "Epoch 00048: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0352 - accuracy: 0.9914 - val_loss: 117899.5078 - val_accuracy: 0.0290\n",
            "Epoch 49/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0581 - accuracy: 0.9881\n",
            "Epoch 00049: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0608 - accuracy: 0.9878 - val_loss: 8554.2139 - val_accuracy: 0.0290\n",
            "Epoch 50/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0446 - accuracy: 0.9877\n",
            "Epoch 00050: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0448 - accuracy: 0.9875 - val_loss: 3336.8459 - val_accuracy: 0.0194\n",
            "Epoch 51/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.9802\n",
            "Epoch 00051: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0803 - accuracy: 0.9803 - val_loss: 36302.0391 - val_accuracy: 0.0323\n",
            "Epoch 52/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0959 - accuracy: 0.9757\n",
            "Epoch 00052: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0946 - accuracy: 0.9760 - val_loss: 94958.3281 - val_accuracy: 0.0323\n",
            "Epoch 53/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0956 - accuracy: 0.9779\n",
            "Epoch 00053: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0966 - accuracy: 0.9778 - val_loss: 25340.5820 - val_accuracy: 0.0323\n",
            "Epoch 54/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0966 - accuracy: 0.9733\n",
            "Epoch 00054: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0947 - accuracy: 0.9738 - val_loss: 14950.9238 - val_accuracy: 0.0387\n",
            "Epoch 55/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0886 - accuracy: 0.9751\n",
            "Epoch 00055: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0902 - accuracy: 0.9746 - val_loss: 28594.3008 - val_accuracy: 0.0323\n",
            "Epoch 56/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9860\n",
            "Epoch 00056: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0575 - accuracy: 0.9860 - val_loss: 28019.6543 - val_accuracy: 0.0226\n",
            "Epoch 57/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0365 - accuracy: 0.9917\n",
            "Epoch 00057: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0365 - accuracy: 0.9918 - val_loss: 9935.5361 - val_accuracy: 0.0290\n",
            "Epoch 58/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0192 - accuracy: 0.9955\n",
            "Epoch 00058: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0190 - accuracy: 0.9957 - val_loss: 22865.4668 - val_accuracy: 0.0323\n",
            "Epoch 59/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0133 - accuracy: 0.9967\n",
            "Epoch 00059: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0130 - accuracy: 0.9968 - val_loss: 14305.6875 - val_accuracy: 0.0323\n",
            "Epoch 60/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0112 - accuracy: 0.9960\n",
            "Epoch 00060: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0110 - accuracy: 0.9961 - val_loss: 34758.8242 - val_accuracy: 0.0323\n",
            "Epoch 61/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0083 - accuracy: 0.9977\n",
            "Epoch 00061: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0084 - accuracy: 0.9975 - val_loss: 40431.9414 - val_accuracy: 0.0323\n",
            "Epoch 62/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0050 - accuracy: 0.9992\n",
            "Epoch 00062: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0050 - accuracy: 0.9993 - val_loss: 15579.9385 - val_accuracy: 0.0419\n",
            "Epoch 63/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0134 - accuracy: 0.9985\n",
            "Epoch 00063: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0129 - accuracy: 0.9986 - val_loss: 23100.9258 - val_accuracy: 0.0323\n",
            "Epoch 64/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0066 - accuracy: 0.9996\n",
            "Epoch 00064: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0064 - accuracy: 0.9996 - val_loss: 5470.5054 - val_accuracy: 0.0355\n",
            "Epoch 65/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0122 - accuracy: 0.9989\n",
            "Epoch 00065: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0118 - accuracy: 0.9989 - val_loss: 29510.8711 - val_accuracy: 0.0323\n",
            "Epoch 66/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0114 - accuracy: 0.9970\n",
            "Epoch 00066: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0111 - accuracy: 0.9971 - val_loss: 10073.9834 - val_accuracy: 0.0323\n",
            "Epoch 67/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0266 - accuracy: 0.9957\n",
            "Epoch 00067: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0264 - accuracy: 0.9957 - val_loss: 11138.9746 - val_accuracy: 0.0290\n",
            "Epoch 68/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9902\n",
            "Epoch 00068: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0665 - accuracy: 0.9903 - val_loss: 6206.3867 - val_accuracy: 0.0323\n",
            "Epoch 69/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0935 - accuracy: 0.9798\n",
            "Epoch 00069: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1022 - accuracy: 0.9781 - val_loss: 32766.8477 - val_accuracy: 0.0323\n",
            "Epoch 70/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.1561 - accuracy: 0.9591\n",
            "Epoch 00070: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1552 - accuracy: 0.9591 - val_loss: 105130.8359 - val_accuracy: 0.0355\n",
            "Epoch 71/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.1956 - accuracy: 0.9460\n",
            "Epoch 00071: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1948 - accuracy: 0.9459 - val_loss: 20223.7383 - val_accuracy: 0.0258\n",
            "Epoch 72/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.1127 - accuracy: 0.9659\n",
            "Epoch 00072: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1115 - accuracy: 0.9663 - val_loss: 8714.7383 - val_accuracy: 0.0323\n",
            "Epoch 73/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0555 - accuracy: 0.9843\n",
            "Epoch 00073: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0551 - accuracy: 0.9842 - val_loss: 22143.2227 - val_accuracy: 0.0323\n",
            "Epoch 74/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0433 - accuracy: 0.9880\n",
            "Epoch 00074: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0438 - accuracy: 0.9878 - val_loss: 4070.0110 - val_accuracy: 0.0290\n",
            "Epoch 75/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9932\n",
            "Epoch 00075: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0285 - accuracy: 0.9932 - val_loss: 6047.7573 - val_accuracy: 0.0323\n",
            "Epoch 76/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0176 - accuracy: 0.9966\n",
            "Epoch 00076: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0171 - accuracy: 0.9968 - val_loss: 10286.5830 - val_accuracy: 0.0290\n",
            "Epoch 77/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9971\n",
            "Epoch 00077: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0148 - accuracy: 0.9971 - val_loss: 76499.6875 - val_accuracy: 0.0323\n",
            "Epoch 78/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9996\n",
            "Epoch 00078: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0045 - accuracy: 0.9996 - val_loss: 34817.4297 - val_accuracy: 0.0323\n",
            "Epoch 79/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0048 - accuracy: 0.9985\n",
            "Epoch 00079: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 43040.5273 - val_accuracy: 0.0323\n",
            "Epoch 80/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 0.9996\n",
            "Epoch 00080: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0067 - accuracy: 0.9996 - val_loss: 45.2849 - val_accuracy: 0.0258\n",
            "Epoch 81/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 00081: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 16.2326 - val_accuracy: 0.0452\n",
            "Epoch 82/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 00082: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 15.4745 - val_accuracy: 0.0355\n",
            "Epoch 83/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 00083: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 13.3521 - val_accuracy: 0.0387\n",
            "Epoch 84/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 00084: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 15.9922 - val_accuracy: 0.0419\n",
            "Epoch 85/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 8.9299e-04 - accuracy: 1.0000\n",
            "Epoch 00085: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 8.8534e-04 - accuracy: 1.0000 - val_loss: 40.1731 - val_accuracy: 0.0290\n",
            "Epoch 86/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 8.2016e-04 - accuracy: 1.0000\n",
            "Epoch 00086: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 8.1945e-04 - accuracy: 1.0000 - val_loss: 15.1763 - val_accuracy: 0.0258\n",
            "Epoch 87/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 7.2487e-04 - accuracy: 1.0000\n",
            "Epoch 00087: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 7.1052e-04 - accuracy: 1.0000 - val_loss: 15.9646 - val_accuracy: 0.0419\n",
            "Epoch 88/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 8.2775e-04 - accuracy: 1.0000\n",
            "Epoch 00088: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 8.6195e-04 - accuracy: 1.0000 - val_loss: 14.7033 - val_accuracy: 0.0419\n",
            "Epoch 89/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9996\n",
            "Epoch 00089: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 178.5827 - val_accuracy: 0.0258\n",
            "Epoch 90/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 8.5600e-04 - accuracy: 1.0000\n",
            "Epoch 00090: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 8.4756e-04 - accuracy: 1.0000 - val_loss: 13.5246 - val_accuracy: 0.0452\n",
            "Epoch 91/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 9.7738e-04 - accuracy: 1.0000\n",
            "Epoch 00091: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 9.5248e-04 - accuracy: 1.0000 - val_loss: 14.1010 - val_accuracy: 0.0387\n",
            "Epoch 92/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 8.1646e-04 - accuracy: 1.0000\n",
            "Epoch 00092: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 8.2976e-04 - accuracy: 1.0000 - val_loss: 2882.0513 - val_accuracy: 0.0258\n",
            "Epoch 93/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 5.9117e-04 - accuracy: 1.0000\n",
            "Epoch 00093: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 5.9909e-04 - accuracy: 1.0000 - val_loss: 15.6454 - val_accuracy: 0.0387\n",
            "Epoch 94/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 5.4304e-04 - accuracy: 1.0000\n",
            "Epoch 00094: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 5.4137e-04 - accuracy: 1.0000 - val_loss: 30.6462 - val_accuracy: 0.0355\n",
            "Epoch 95/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 5.1445e-04 - accuracy: 1.0000\n",
            "Epoch 00095: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 5.2669e-04 - accuracy: 1.0000 - val_loss: 18.4026 - val_accuracy: 0.0387\n",
            "Epoch 96/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 6.6410e-04 - accuracy: 1.0000\n",
            "Epoch 00096: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 6.5724e-04 - accuracy: 1.0000 - val_loss: 124.5919 - val_accuracy: 0.0323\n",
            "Epoch 97/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0059 - accuracy: 0.9985\n",
            "Epoch 00097: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0126 - accuracy: 0.9978 - val_loss: 7577.8613 - val_accuracy: 0.0323\n",
            "Epoch 98/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.2182 - accuracy: 0.9562\n",
            "Epoch 00098: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.2178 - accuracy: 0.9559 - val_loss: 10326.4980 - val_accuracy: 0.0323\n",
            "Epoch 99/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.5276 - accuracy: 0.8604\n",
            "Epoch 00099: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.5253 - accuracy: 0.8609 - val_loss: 425864.5312 - val_accuracy: 0.0323\n",
            "Epoch 100/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.3741 - accuracy: 0.8953\n",
            "Epoch 00100: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.3769 - accuracy: 0.8943 - val_loss: 47643.1758 - val_accuracy: 0.0323\n",
            "Epoch 101/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.1384 - accuracy: 0.9577\n",
            "Epoch 00101: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1384 - accuracy: 0.9577 - val_loss: 3784.7795 - val_accuracy: 0.0323\n",
            "Epoch 102/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9935\n",
            "Epoch 00102: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0318 - accuracy: 0.9935 - val_loss: 1802.5391 - val_accuracy: 0.0355\n",
            "Epoch 103/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9975\n",
            "Epoch 00103: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0135 - accuracy: 0.9975 - val_loss: 2148.8804 - val_accuracy: 0.0226\n",
            "Epoch 104/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0097 - accuracy: 0.9993\n",
            "Epoch 00104: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0097 - accuracy: 0.9993 - val_loss: 13.8789 - val_accuracy: 0.0484\n",
            "Epoch 105/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 00105: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 15.0297 - val_accuracy: 0.0516\n",
            "Epoch 106/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0042 - accuracy: 0.9996\n",
            "Epoch 00106: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0042 - accuracy: 0.9996 - val_loss: 82.9698 - val_accuracy: 0.0290\n",
            "Epoch 107/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 00107: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 15.2749 - val_accuracy: 0.0516\n",
            "Epoch 108/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 00108: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 15.4705 - val_accuracy: 0.0452\n",
            "Epoch 109/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 00109: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 15.3947 - val_accuracy: 0.0516\n",
            "Epoch 110/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 00110: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 48.7313 - val_accuracy: 0.0323\n",
            "Epoch 111/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 00111: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 14.8497 - val_accuracy: 0.0452\n",
            "Epoch 112/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 00112: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 14.9929 - val_accuracy: 0.0484\n",
            "Epoch 113/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 00113: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 15.1443 - val_accuracy: 0.0548\n",
            "Epoch 114/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 00114: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 15.4354 - val_accuracy: 0.0516\n",
            "Epoch 115/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 00115: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 12ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 15.0838 - val_accuracy: 0.0516\n",
            "Epoch 116/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 00116: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 14.9731 - val_accuracy: 0.0516\n",
            "Epoch 117/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 00117: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 137.1550 - val_accuracy: 0.0226\n",
            "Epoch 118/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 00118: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 15.7635 - val_accuracy: 0.0484\n",
            "Epoch 119/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 00119: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 16.2038 - val_accuracy: 0.0387\n",
            "Epoch 120/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 00120: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 16.1423 - val_accuracy: 0.0484\n",
            "Epoch 121/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 9.9604e-04 - accuracy: 1.0000\n",
            "Epoch 00121: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 9.9984e-04 - accuracy: 1.0000 - val_loss: 15.2626 - val_accuracy: 0.0548\n",
            "Epoch 122/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 7.4996e-04 - accuracy: 1.0000\n",
            "Epoch 00122: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 7.4996e-04 - accuracy: 1.0000 - val_loss: 15.6510 - val_accuracy: 0.0516\n",
            "Epoch 123/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 8.0109e-04 - accuracy: 1.0000\n",
            "Epoch 00123: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 8.1081e-04 - accuracy: 1.0000 - val_loss: 16.1497 - val_accuracy: 0.0484\n",
            "Epoch 124/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 9.8797e-04 - accuracy: 1.0000\n",
            "Epoch 00124: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 9.8018e-04 - accuracy: 1.0000 - val_loss: 138.3949 - val_accuracy: 0.0323\n",
            "Epoch 125/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 7.2764e-04 - accuracy: 1.0000\n",
            "Epoch 00125: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 7.1748e-04 - accuracy: 1.0000 - val_loss: 15.4832 - val_accuracy: 0.0484\n",
            "Epoch 126/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 7.7556e-04 - accuracy: 1.0000\n",
            "Epoch 00126: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 7.6248e-04 - accuracy: 1.0000 - val_loss: 15.8911 - val_accuracy: 0.0484\n",
            "Epoch 127/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 6.1456e-04 - accuracy: 1.0000\n",
            "Epoch 00127: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 6.1456e-04 - accuracy: 1.0000 - val_loss: 15.3457 - val_accuracy: 0.0548\n",
            "Epoch 128/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 6.0807e-04 - accuracy: 1.0000\n",
            "Epoch 00128: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 6.1394e-04 - accuracy: 1.0000 - val_loss: 15.5410 - val_accuracy: 0.0516\n",
            "Epoch 129/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 6.2138e-04 - accuracy: 1.0000\n",
            "Epoch 00129: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 6.2138e-04 - accuracy: 1.0000 - val_loss: 15.5687 - val_accuracy: 0.0452\n",
            "Epoch 130/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 4.0186e-04 - accuracy: 1.0000\n",
            "Epoch 00130: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 4.0194e-04 - accuracy: 1.0000 - val_loss: 16.0717 - val_accuracy: 0.0516\n",
            "Epoch 131/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 7.4719e-04 - accuracy: 1.0000\n",
            "Epoch 00131: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 7.4719e-04 - accuracy: 1.0000 - val_loss: 16.5872 - val_accuracy: 0.0484\n",
            "Epoch 132/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 5.0514e-04 - accuracy: 1.0000\n",
            "Epoch 00132: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 5.1445e-04 - accuracy: 1.0000 - val_loss: 16.0482 - val_accuracy: 0.0419\n",
            "Epoch 133/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 5.6906e-04 - accuracy: 1.0000\n",
            "Epoch 00133: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 5.6187e-04 - accuracy: 1.0000 - val_loss: 16.3913 - val_accuracy: 0.0419\n",
            "Epoch 134/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 5.1057e-04 - accuracy: 1.0000\n",
            "Epoch 00134: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 5.0590e-04 - accuracy: 1.0000 - val_loss: 16.2861 - val_accuracy: 0.0516\n",
            "Epoch 135/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 6.6898e-04 - accuracy: 1.0000\n",
            "Epoch 00135: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 6.5735e-04 - accuracy: 1.0000 - val_loss: 14.1123 - val_accuracy: 0.0419\n",
            "Epoch 136/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 5.3840e-04 - accuracy: 1.0000\n",
            "Epoch 00136: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 5.3840e-04 - accuracy: 1.0000 - val_loss: 17.2858 - val_accuracy: 0.0258\n",
            "Epoch 137/155\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 4.3674e-04 - accuracy: 1.0000\n",
            "Epoch 00137: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 4.4272e-04 - accuracy: 1.0000 - val_loss: 27.3240 - val_accuracy: 0.0323\n",
            "Epoch 138/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 3.8306e-04 - accuracy: 1.0000\n",
            "Epoch 00138: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 3.8186e-04 - accuracy: 1.0000 - val_loss: 36.8999 - val_accuracy: 0.0323\n",
            "Epoch 139/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 4.7592e-04 - accuracy: 1.0000\n",
            "Epoch 00139: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 4.7449e-04 - accuracy: 1.0000 - val_loss: 15.9593 - val_accuracy: 0.0419\n",
            "Epoch 140/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 3.4130e-04 - accuracy: 1.0000\n",
            "Epoch 00140: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 3.4130e-04 - accuracy: 1.0000 - val_loss: 15.8321 - val_accuracy: 0.0484\n",
            "Epoch 141/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 4.3444e-04 - accuracy: 1.0000\n",
            "Epoch 00141: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 4.3444e-04 - accuracy: 1.0000 - val_loss: 15.6446 - val_accuracy: 0.0484\n",
            "Epoch 142/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9993\n",
            "Epoch 00142: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0071 - accuracy: 0.9993 - val_loss: 3475.9983 - val_accuracy: 0.0323\n",
            "Epoch 143/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.4162 - accuracy: 0.9052\n",
            "Epoch 00143: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.4341 - accuracy: 0.9004 - val_loss: 43010.7734 - val_accuracy: 0.0323\n",
            "Epoch 144/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.7538 - accuracy: 0.8033\n",
            "Epoch 00144: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.7515 - accuracy: 0.8036 - val_loss: 109427.3672 - val_accuracy: 0.0323\n",
            "Epoch 145/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.1929 - accuracy: 0.9355\n",
            "Epoch 00145: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1913 - accuracy: 0.9362 - val_loss: 186204.2969 - val_accuracy: 0.0323\n",
            "Epoch 146/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0478 - accuracy: 0.9862\n",
            "Epoch 00146: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0475 - accuracy: 0.9864 - val_loss: 3367.4141 - val_accuracy: 0.0323\n",
            "Epoch 147/155\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0251 - accuracy: 0.9932\n",
            "Epoch 00147: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0255 - accuracy: 0.9932 - val_loss: 2529.9114 - val_accuracy: 0.0323\n",
            "Epoch 148/155\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0136 - accuracy: 0.9970\n",
            "Epoch 00148: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0133 - accuracy: 0.9971 - val_loss: 16651.2832 - val_accuracy: 0.0355\n",
            "Epoch 149/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0147 - accuracy: 0.9971\n",
            "Epoch 00149: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9971 - val_loss: 13304.3350 - val_accuracy: 0.0355\n",
            "Epoch 150/155\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0063 - accuracy: 0.9989\n",
            "Epoch 00150: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0062 - accuracy: 0.9989 - val_loss: 13.4183 - val_accuracy: 0.0452\n",
            "Epoch 151/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9996\n",
            "Epoch 00151: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0079 - accuracy: 0.9996 - val_loss: 3682.7019 - val_accuracy: 0.0258\n",
            "Epoch 152/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 00152: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 14.9850 - val_accuracy: 0.0452\n",
            "Epoch 153/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 00153: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2423.4985 - val_accuracy: 0.0323\n",
            "Epoch 154/155\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 00154: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 15.0038 - val_accuracy: 0.0452\n",
            "Epoch 155/155\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 00155: val_loss did not improve from 5.39866\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 15.4227 - val_accuracy: 0.0419\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UapZuzdzgHoH",
        "outputId": "d1b29912-671d-43a6-82fc-5f66b7a80bbe"
      },
      "source": [
        "\n",
        "train_accCNN=history.history['accuracy']\n",
        "val_accCNN=history.history['val_accuracy']\n",
        "print(\"Train accuracy:\", train_accCNN[epochs-1])\n",
        "print(\"Validation Accuracy:\", val_accCNN[epochs-1])\n",
        "test_scoresCNN=my_model3.evaluate(xTest, yTest, verbose=2)\n",
        "print(\"Accuracy on Test set:\", test_scoresCNN[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy: 1.0\n",
            "Validation Accuracy: 0.04193548485636711\n",
            "20/20 - 0s - loss: 1307092.3750 - accuracy: 0.0323\n",
            "Accuracy on Test set: 0.032258063554763794\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "4V_ze6mPPAQp",
        "outputId": "9f2173bb-2ff3-4228-b805-e3bf70f3688b"
      },
      "source": [
        "# plot training and validation accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylim([0,1.5])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.savefig(\"CNN2D.png\", dpi=300)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f2c4kd05CS-"
      },
      "source": [
        "### Analysis:\n",
        "\n",
        "1. I have used optimizer like SGD() that did not perform well as the train accuarcy was below 0.40. But, after using adam the accuarcy was improved to 0.70\n",
        "2. The keras seqential model with less than 4 million parameters does not actually perform well. The accuracy was incraesed to above 0.50 after adding 3rd convoluitonal block. \n",
        "3. However, Batch Normalization works better as a regularization method than dropout for this dataset.\n",
        "3. The model accuracy increases slightly  with further training and increase in the numebr of epochs\n",
        "4. As it is seen the model accuarcy on the tarining set is 1 whereas the accuracy on valid and test set is at 0.0419 and 0.0322 respectively. So, model did not work well on validation and test data which is evident in the learning curve.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wn9RIk8zu55"
      },
      "source": [
        "# Section 4: Regression Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaR37S8e_w_S"
      },
      "source": [
        "### Mapping the class labels in between -1 to 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz0uk8sPxKbq",
        "outputId": "2ef19153-0c6a-45b1-f367-a051a329868c"
      },
      "source": [
        "#preparing labels for regressor model and storing the new labels in yTrainN, yValidN, yTestN\n",
        "import numpy as np\n",
        "yTrainN=np.array(angle_DiffTrain)\n",
        "yValidN=np.array(angle_DiffValid)\n",
        "yTestN=np.array(angle_DiffTest)\n",
        "\n",
        "#Mapping the labels in between -1 to 1 \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "yTrainN=scaler.fit_transform(yTrainN.reshape(-1, 1))\n",
        "yValidN=scaler.fit_transform(yValidN.reshape(-1, 1))\n",
        "yTestN = scaler.fit_transform(yTestN.reshape(-1, 1))\n",
        "print(\" ytrain shape:\", yTrainN.shape)\n",
        "print(\"Checking the mapped class labels in yTrain:\", np.unique(yTrainN))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ytrain shape: (2790, 1)\n",
            "Checking the mapped class labels in yTrain: [-1.         -0.93333333 -0.86666667 -0.8        -0.73333333 -0.66666667\n",
            " -0.6        -0.53333333 -0.46666667 -0.4        -0.33333333 -0.26666667\n",
            " -0.2        -0.13333333 -0.06666667  0.          0.06666667  0.13333333\n",
            "  0.2         0.26666667  0.33333333  0.4         0.46666667  0.53333333\n",
            "  0.6         0.66666667  0.73333333  0.8         0.86666667  0.93333333\n",
            "  1.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUF2qRuq25Xg",
        "outputId": "84991975-b6f7-4964-b1aa-56072dc7e337"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "#define model\n",
        "regressor_model =Sequential()\n",
        "\n",
        "# 1st block \n",
        "regressor_model.add(Conv2D(32, (3, 3), activation='relu', padding='same', \n",
        "                    input_shape=(80, 50, 6)))\n",
        "regressor_model.add(BatchNormalization())\n",
        "regressor_model.add(MaxPooling2D((2, 2), padding='same'))\n",
        "\n",
        "# Second block\n",
        "regressor_model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "regressor_model.add(BatchNormalization())\n",
        "regressor_model.add(MaxPooling2D((2, 2), padding='same'))\n",
        "\n",
        "#3rd block \n",
        "regressor_model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "regressor_model.add(BatchNormalization())\n",
        "regressor_model.add(MaxPooling2D((2, 2), padding='same'))\n",
        "\n",
        "# model.add(dropout())  # this converts our 3D feature maps to 1D feature vectors\n",
        "regressor_model.add(Flatten())\n",
        "# fully connected layer\n",
        "regressor_model.add(Dense(512, activation='relu'))\n",
        "regressor_model.add(BatchNormalization())\n",
        "# make predictions\n",
        "regressor_model.add(Dense(1, activation='tanh'))\n",
        "\n",
        "# Show a summary of the model. Check the number of trainable parameters\n",
        "regressor_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 80, 50, 32)        1760      \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 80, 50, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 40, 25, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 40, 25, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 40, 25, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 20, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 20, 13, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 20, 13, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 10, 7, 128)        0         \n",
            "_________________________________________________________________\n",
            "flatten_17 (Flatten)         (None, 8960)              0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 512)               4588032   \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 4,685,601\n",
            "Trainable params: 4,684,129\n",
            "Non-trainable params: 1,472\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmMpLmW6StdT"
      },
      "source": [
        "# use early stopping to optimally terminate training through callbacks\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=153)\n",
        "\n",
        "# save best model automatically\n",
        "mc= ModelCheckpoint('yourdirectory/regressorCNN_model.h5', monitor='val_loss', \n",
        "                    mode='min', verbose=1, save_best_only=True)\n",
        "cb_list=[es,mc]\n",
        "#00001\n",
        "from keras import optimizers\n",
        "optimizer_new=optimizers.RMSprop(lr=0.0005) \n",
        "regressor_model.compile(optimizer=optimizer_new, loss='mse', \n",
        "                 metrics=['mse'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66xoQ3zytE9k"
      },
      "source": [
        "#Creating dataGenerator \n",
        "import tensorflow as tf\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th38cnkDsoqx",
        "outputId": "e28af888-5207-44bc-ee25-e533a09017fa"
      },
      "source": [
        "history = regressor_model.fit(\n",
        "        xTrain, yTrainN, \n",
        "        batch_size=30,\n",
        "        epochs=160,\n",
        "        validation_data=(xValid, yValidN),\n",
        "        callbacks=cb_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.3297 - mse: 0.3297\n",
            "Epoch 00001: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.3285 - mse: 0.3285 - val_loss: 0.6917 - val_mse: 0.6917\n",
            "Epoch 2/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.2876 - mse: 0.2876\n",
            "Epoch 00002: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.2887 - mse: 0.2887 - val_loss: 0.9606 - val_mse: 0.9606\n",
            "Epoch 3/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.2656 - mse: 0.2656\n",
            "Epoch 00003: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.5375 - val_mse: 0.5375\n",
            "Epoch 4/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.2347 - mse: 0.2347\n",
            "Epoch 00004: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.2351 - mse: 0.2351 - val_loss: 0.1522 - val_mse: 0.1522\n",
            "Epoch 5/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.2287 - mse: 0.2287\n",
            "Epoch 00005: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.2288 - mse: 0.2288 - val_loss: 0.2083 - val_mse: 0.2083\n",
            "Epoch 6/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.2122 - mse: 0.2122\n",
            "Epoch 00006: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.2119 - mse: 0.2119 - val_loss: 0.8279 - val_mse: 0.8279\n",
            "Epoch 7/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.2137 - mse: 0.2137\n",
            "Epoch 00007: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.2152 - mse: 0.2152 - val_loss: 0.5079 - val_mse: 0.5079\n",
            "Epoch 8/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.2003 - mse: 0.2003\n",
            "Epoch 00008: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1996 - mse: 0.1996 - val_loss: 0.4909 - val_mse: 0.4909\n",
            "Epoch 9/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.1926 - mse: 0.1926\n",
            "Epoch 00009: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1928 - mse: 0.1928 - val_loss: 0.2985 - val_mse: 0.2985\n",
            "Epoch 10/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.1869 - mse: 0.1869\n",
            "Epoch 00010: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1867 - mse: 0.1867 - val_loss: 0.2165 - val_mse: 0.2165\n",
            "Epoch 11/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.1820 - mse: 0.1820\n",
            "Epoch 00011: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1822 - mse: 0.1822 - val_loss: 0.1030 - val_mse: 0.1030\n",
            "Epoch 12/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.1742 - mse: 0.1742\n",
            "Epoch 00012: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1736 - mse: 0.1736 - val_loss: 0.3401 - val_mse: 0.3401\n",
            "Epoch 13/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.1756 - mse: 0.1756\n",
            "Epoch 00013: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1753 - mse: 0.1753 - val_loss: 1.0067 - val_mse: 1.0067\n",
            "Epoch 14/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.1670 - mse: 0.1670\n",
            "Epoch 00014: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1663 - mse: 0.1663 - val_loss: 0.1740 - val_mse: 0.1740\n",
            "Epoch 15/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.1622 - mse: 0.1622\n",
            "Epoch 00015: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1624 - mse: 0.1624 - val_loss: 0.3013 - val_mse: 0.3013\n",
            "Epoch 16/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.1577 - mse: 0.1577\n",
            "Epoch 00016: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1572 - mse: 0.1572 - val_loss: 0.4349 - val_mse: 0.4349\n",
            "Epoch 17/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.1545 - mse: 0.1545\n",
            "Epoch 00017: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1549 - mse: 0.1549 - val_loss: 0.2161 - val_mse: 0.2161\n",
            "Epoch 18/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.1478 - mse: 0.1478\n",
            "Epoch 00018: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1479 - mse: 0.1479 - val_loss: 0.2242 - val_mse: 0.2242\n",
            "Epoch 19/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.1480 - mse: 0.1480\n",
            "Epoch 00019: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1478 - mse: 0.1478 - val_loss: 0.3632 - val_mse: 0.3632\n",
            "Epoch 20/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.1379 - mse: 0.1379\n",
            "Epoch 00020: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1383 - mse: 0.1383 - val_loss: 0.2635 - val_mse: 0.2635\n",
            "Epoch 21/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.1435 - mse: 0.1435\n",
            "Epoch 00021: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1435 - mse: 0.1435 - val_loss: 0.3939 - val_mse: 0.3939\n",
            "Epoch 22/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.1317 - mse: 0.1317\n",
            "Epoch 00022: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1319 - mse: 0.1319 - val_loss: 0.4854 - val_mse: 0.4854\n",
            "Epoch 23/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.1316 - mse: 0.1316\n",
            "Epoch 00023: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1316 - mse: 0.1316 - val_loss: 0.3966 - val_mse: 0.3966\n",
            "Epoch 24/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.1240 - mse: 0.1240\n",
            "Epoch 00024: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1238 - mse: 0.1238 - val_loss: 0.3028 - val_mse: 0.3028\n",
            "Epoch 25/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.1231 - mse: 0.1231\n",
            "Epoch 00025: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1227 - mse: 0.1227 - val_loss: 0.3492 - val_mse: 0.3492\n",
            "Epoch 26/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.1199 - mse: 0.1199\n",
            "Epoch 00026: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1204 - mse: 0.1204 - val_loss: 0.3290 - val_mse: 0.3290\n",
            "Epoch 27/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.1186 - mse: 0.1186\n",
            "Epoch 00027: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1181 - mse: 0.1181 - val_loss: 0.3198 - val_mse: 0.3198\n",
            "Epoch 28/160\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.1170 - mse: 0.1170\n",
            "Epoch 00028: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1172 - mse: 0.1172 - val_loss: 0.1988 - val_mse: 0.1988\n",
            "Epoch 29/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.1121 - mse: 0.1121\n",
            "Epoch 00029: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1110 - mse: 0.1110 - val_loss: 0.3173 - val_mse: 0.3173\n",
            "Epoch 30/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.1124 - mse: 0.1124\n",
            "Epoch 00030: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1105 - mse: 0.1105 - val_loss: 0.3728 - val_mse: 0.3728\n",
            "Epoch 31/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.1080 - mse: 0.1080\n",
            "Epoch 00031: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1093 - mse: 0.1093 - val_loss: 0.3811 - val_mse: 0.3811\n",
            "Epoch 32/160\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.1024 - mse: 0.1024\n",
            "Epoch 00032: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1040 - mse: 0.1040 - val_loss: 1.0968 - val_mse: 1.0968\n",
            "Epoch 33/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0982 - mse: 0.0982\n",
            "Epoch 00033: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.2719 - val_mse: 0.2719\n",
            "Epoch 34/160\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.1002 - mse: 0.1002\n",
            "Epoch 00034: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.1002 - mse: 0.1002 - val_loss: 0.7359 - val_mse: 0.7359\n",
            "Epoch 35/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0924 - mse: 0.0924\n",
            "Epoch 00035: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0924 - mse: 0.0924 - val_loss: 0.2204 - val_mse: 0.2204\n",
            "Epoch 36/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0956 - mse: 0.0956\n",
            "Epoch 00036: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0958 - mse: 0.0958 - val_loss: 0.3264 - val_mse: 0.3264\n",
            "Epoch 37/160\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0872 - mse: 0.0872\n",
            "Epoch 00037: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0872 - mse: 0.0872 - val_loss: 0.2844 - val_mse: 0.2844\n",
            "Epoch 38/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0924 - mse: 0.0924\n",
            "Epoch 00038: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0928 - mse: 0.0928 - val_loss: 0.4054 - val_mse: 0.4054\n",
            "Epoch 39/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0892 - mse: 0.0892\n",
            "Epoch 00039: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.3156 - val_mse: 0.3156\n",
            "Epoch 40/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0873 - mse: 0.0873\n",
            "Epoch 00040: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0880 - mse: 0.0880 - val_loss: 0.3995 - val_mse: 0.3995\n",
            "Epoch 41/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0853 - mse: 0.0853\n",
            "Epoch 00041: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0853 - mse: 0.0853 - val_loss: 0.4087 - val_mse: 0.4087\n",
            "Epoch 42/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0854 - mse: 0.0854\n",
            "Epoch 00042: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0848 - mse: 0.0848 - val_loss: 0.3489 - val_mse: 0.3489\n",
            "Epoch 43/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0806 - mse: 0.0806\n",
            "Epoch 00043: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0809 - mse: 0.0809 - val_loss: 0.4871 - val_mse: 0.4871\n",
            "Epoch 44/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0755 - mse: 0.0755\n",
            "Epoch 00044: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0748 - mse: 0.0748 - val_loss: 0.3171 - val_mse: 0.3171\n",
            "Epoch 45/160\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0796 - mse: 0.0796\n",
            "Epoch 00045: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0796 - mse: 0.0796 - val_loss: 0.3690 - val_mse: 0.3690\n",
            "Epoch 46/160\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0742 - mse: 0.0742\n",
            "Epoch 00046: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0742 - mse: 0.0742 - val_loss: 0.3281 - val_mse: 0.3281\n",
            "Epoch 47/160\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0740 - mse: 0.0740\n",
            "Epoch 00047: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0739 - mse: 0.0739 - val_loss: 0.4087 - val_mse: 0.4087\n",
            "Epoch 48/160\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0739 - mse: 0.0739\n",
            "Epoch 00048: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0743 - mse: 0.0743 - val_loss: 0.3145 - val_mse: 0.3145\n",
            "Epoch 49/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0729 - mse: 0.0729\n",
            "Epoch 00049: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.3198 - val_mse: 0.3198\n",
            "Epoch 50/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0697 - mse: 0.0697\n",
            "Epoch 00050: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0700 - mse: 0.0700 - val_loss: 0.3145 - val_mse: 0.3145\n",
            "Epoch 51/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0628 - mse: 0.0628\n",
            "Epoch 00051: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0624 - mse: 0.0624 - val_loss: 0.5027 - val_mse: 0.5027\n",
            "Epoch 52/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0651 - mse: 0.0651\n",
            "Epoch 00052: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0647 - mse: 0.0647 - val_loss: 0.3461 - val_mse: 0.3461\n",
            "Epoch 53/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0697 - mse: 0.0697\n",
            "Epoch 00053: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0700 - mse: 0.0700 - val_loss: 0.3319 - val_mse: 0.3319\n",
            "Epoch 54/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0690 - mse: 0.0690\n",
            "Epoch 00054: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0688 - mse: 0.0688 - val_loss: 0.2578 - val_mse: 0.2578\n",
            "Epoch 55/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0638 - mse: 0.0638\n",
            "Epoch 00055: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0640 - mse: 0.0640 - val_loss: 0.3168 - val_mse: 0.3168\n",
            "Epoch 56/160\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0629 - mse: 0.0629\n",
            "Epoch 00056: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0629 - mse: 0.0629 - val_loss: 0.3070 - val_mse: 0.3070\n",
            "Epoch 57/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0593 - mse: 0.0593\n",
            "Epoch 00057: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0595 - mse: 0.0595 - val_loss: 0.4343 - val_mse: 0.4343\n",
            "Epoch 58/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0591 - mse: 0.0591\n",
            "Epoch 00058: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0594 - mse: 0.0594 - val_loss: 0.3806 - val_mse: 0.3806\n",
            "Epoch 59/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0645 - mse: 0.0645\n",
            "Epoch 00059: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0648 - mse: 0.0648 - val_loss: 0.3669 - val_mse: 0.3669\n",
            "Epoch 60/160\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0615 - mse: 0.0615\n",
            "Epoch 00060: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0615 - mse: 0.0615 - val_loss: 0.3532 - val_mse: 0.3532\n",
            "Epoch 61/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0608 - mse: 0.0608\n",
            "Epoch 00061: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0603 - mse: 0.0603 - val_loss: 0.4289 - val_mse: 0.4289\n",
            "Epoch 62/160\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0599 - mse: 0.0599\n",
            "Epoch 00062: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0599 - mse: 0.0599 - val_loss: 0.3789 - val_mse: 0.3789\n",
            "Epoch 63/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0582 - mse: 0.0582\n",
            "Epoch 00063: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0578 - mse: 0.0578 - val_loss: 0.3898 - val_mse: 0.3898\n",
            "Epoch 64/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0598 - mse: 0.0598\n",
            "Epoch 00064: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0601 - mse: 0.0601 - val_loss: 0.2647 - val_mse: 0.2647\n",
            "Epoch 65/160\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0556 - mse: 0.0556\n",
            "Epoch 00065: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0556 - mse: 0.0556 - val_loss: 0.4270 - val_mse: 0.4270\n",
            "Epoch 66/160\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0554 - mse: 0.0554\n",
            "Epoch 00066: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0550 - mse: 0.0550 - val_loss: 0.3205 - val_mse: 0.3205\n",
            "Epoch 67/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0534 - mse: 0.0534\n",
            "Epoch 00067: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0543 - mse: 0.0543 - val_loss: 0.4653 - val_mse: 0.4653\n",
            "Epoch 68/160\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0553 - mse: 0.0553\n",
            "Epoch 00068: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0553 - mse: 0.0553 - val_loss: 0.3564 - val_mse: 0.3564\n",
            "Epoch 69/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0576 - mse: 0.0576\n",
            "Epoch 00069: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0576 - mse: 0.0576 - val_loss: 0.3882 - val_mse: 0.3882\n",
            "Epoch 70/160\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0516 - mse: 0.0516\n",
            "Epoch 00070: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0524 - mse: 0.0524 - val_loss: 0.3823 - val_mse: 0.3823\n",
            "Epoch 71/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0535 - mse: 0.0535\n",
            "Epoch 00071: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0534 - mse: 0.0534 - val_loss: 0.4649 - val_mse: 0.4649\n",
            "Epoch 72/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0515 - mse: 0.0515\n",
            "Epoch 00072: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0525 - mse: 0.0525 - val_loss: 0.3568 - val_mse: 0.3568\n",
            "Epoch 73/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0472 - mse: 0.0472\n",
            "Epoch 00073: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0475 - mse: 0.0475 - val_loss: 0.3280 - val_mse: 0.3280\n",
            "Epoch 74/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0492 - mse: 0.0492\n",
            "Epoch 00074: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0496 - mse: 0.0496 - val_loss: 0.2840 - val_mse: 0.2840\n",
            "Epoch 75/160\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0525 - mse: 0.0525\n",
            "Epoch 00075: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0526 - mse: 0.0526 - val_loss: 0.4160 - val_mse: 0.4160\n",
            "Epoch 76/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0514 - mse: 0.0514\n",
            "Epoch 00076: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0511 - mse: 0.0511 - val_loss: 0.3515 - val_mse: 0.3515\n",
            "Epoch 77/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0488 - mse: 0.0488\n",
            "Epoch 00077: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0489 - mse: 0.0489 - val_loss: 0.3994 - val_mse: 0.3994\n",
            "Epoch 78/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0492 - mse: 0.0492\n",
            "Epoch 00078: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.4433 - val_mse: 0.4433\n",
            "Epoch 79/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0499 - mse: 0.0499\n",
            "Epoch 00079: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.3413 - val_mse: 0.3413\n",
            "Epoch 80/160\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0479 - mse: 0.0479\n",
            "Epoch 00080: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0479 - mse: 0.0479 - val_loss: 0.3217 - val_mse: 0.3217\n",
            "Epoch 81/160\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0449 - mse: 0.0449\n",
            "Epoch 00081: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0450 - mse: 0.0450 - val_loss: 0.2848 - val_mse: 0.2848\n",
            "Epoch 82/160\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0460 - mse: 0.0460\n",
            "Epoch 00082: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0460 - mse: 0.0460 - val_loss: 0.4234 - val_mse: 0.4234\n",
            "Epoch 83/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0438 - mse: 0.0438\n",
            "Epoch 00083: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0439 - mse: 0.0439 - val_loss: 0.3567 - val_mse: 0.3567\n",
            "Epoch 84/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0457 - mse: 0.0457\n",
            "Epoch 00084: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0458 - mse: 0.0458 - val_loss: 0.3091 - val_mse: 0.3091\n",
            "Epoch 85/160\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0472 - mse: 0.0472\n",
            "Epoch 00085: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0472 - mse: 0.0472 - val_loss: 0.4143 - val_mse: 0.4143\n",
            "Epoch 86/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0434 - mse: 0.0434\n",
            "Epoch 00086: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0438 - mse: 0.0438 - val_loss: 0.3837 - val_mse: 0.3837\n",
            "Epoch 87/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0448 - mse: 0.0448\n",
            "Epoch 00087: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0444 - mse: 0.0444 - val_loss: 0.2982 - val_mse: 0.2982\n",
            "Epoch 88/160\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0444 - mse: 0.0444\n",
            "Epoch 00088: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0444 - mse: 0.0444 - val_loss: 0.3149 - val_mse: 0.3149\n",
            "Epoch 89/160\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0464 - mse: 0.0464\n",
            "Epoch 00089: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0463 - mse: 0.0463 - val_loss: 0.3420 - val_mse: 0.3420\n",
            "Epoch 90/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0412 - mse: 0.0412\n",
            "Epoch 00090: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.3841 - val_mse: 0.3841\n",
            "Epoch 91/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0437 - mse: 0.0437\n",
            "Epoch 00091: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0432 - mse: 0.0432 - val_loss: 0.2888 - val_mse: 0.2888\n",
            "Epoch 92/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0445 - mse: 0.0445\n",
            "Epoch 00092: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0443 - mse: 0.0443 - val_loss: 0.3484 - val_mse: 0.3484\n",
            "Epoch 93/160\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 00093: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.3267 - val_mse: 0.3267\n",
            "Epoch 94/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0420 - mse: 0.0420\n",
            "Epoch 00094: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0422 - mse: 0.0422 - val_loss: 0.3936 - val_mse: 0.3936\n",
            "Epoch 95/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0435 - mse: 0.0435\n",
            "Epoch 00095: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0432 - mse: 0.0432 - val_loss: 0.4056 - val_mse: 0.4056\n",
            "Epoch 96/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0403 - mse: 0.0403\n",
            "Epoch 00096: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.3350 - val_mse: 0.3350\n",
            "Epoch 97/160\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 00097: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.3039 - val_mse: 0.3039\n",
            "Epoch 98/160\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0355 - mse: 0.0355\n",
            "Epoch 00098: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.3457 - val_mse: 0.3457\n",
            "Epoch 99/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 00099: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.2558 - val_mse: 0.2558\n",
            "Epoch 100/160\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0384 - mse: 0.0384\n",
            "Epoch 00100: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.2693 - val_mse: 0.2693\n",
            "Epoch 101/160\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0434 - mse: 0.0434\n",
            "Epoch 00101: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.3641 - val_mse: 0.3641\n",
            "Epoch 102/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
            "Epoch 00102: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.3746 - val_mse: 0.3746\n",
            "Epoch 103/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0403 - mse: 0.0403\n",
            "Epoch 00103: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0404 - mse: 0.0404 - val_loss: 0.2928 - val_mse: 0.2928\n",
            "Epoch 104/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 00104: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.4002 - val_mse: 0.4002\n",
            "Epoch 105/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 00105: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.3769 - val_mse: 0.3769\n",
            "Epoch 106/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0369 - mse: 0.0369\n",
            "Epoch 00106: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.2969 - val_mse: 0.2969\n",
            "Epoch 107/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0368 - mse: 0.0368\n",
            "Epoch 00107: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.3745 - val_mse: 0.3745\n",
            "Epoch 108/160\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 00108: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.3645 - val_mse: 0.3645\n",
            "Epoch 109/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0340 - mse: 0.0340\n",
            "Epoch 00109: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.3509 - val_mse: 0.3509\n",
            "Epoch 110/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0374 - mse: 0.0374\n",
            "Epoch 00110: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.3602 - val_mse: 0.3602\n",
            "Epoch 111/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0342 - mse: 0.0342\n",
            "Epoch 00111: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.3157 - val_mse: 0.3157\n",
            "Epoch 112/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 00112: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.3809 - val_mse: 0.3809\n",
            "Epoch 113/160\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0379 - mse: 0.0379\n",
            "Epoch 00113: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.3111 - val_mse: 0.3111\n",
            "Epoch 114/160\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0361 - mse: 0.0361\n",
            "Epoch 00114: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.3360 - val_mse: 0.3360\n",
            "Epoch 115/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0357 - mse: 0.0357\n",
            "Epoch 00115: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.3780 - val_mse: 0.3780\n",
            "Epoch 116/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0364 - mse: 0.0364\n",
            "Epoch 00116: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.3965 - val_mse: 0.3965\n",
            "Epoch 117/160\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0350 - mse: 0.0350\n",
            "Epoch 00117: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0350 - mse: 0.0350 - val_loss: 0.3779 - val_mse: 0.3779\n",
            "Epoch 118/160\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0330 - mse: 0.0330\n",
            "Epoch 00118: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.3234 - val_mse: 0.3234\n",
            "Epoch 119/160\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0336 - mse: 0.0336\n",
            "Epoch 00119: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.3900 - val_mse: 0.3900\n",
            "Epoch 120/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0336 - mse: 0.0336\n",
            "Epoch 00120: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.3781 - val_mse: 0.3781\n",
            "Epoch 121/160\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0311 - mse: 0.0311\n",
            "Epoch 00121: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.3323 - val_mse: 0.3323\n",
            "Epoch 122/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0334 - mse: 0.0334\n",
            "Epoch 00122: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.3585 - val_mse: 0.3585\n",
            "Epoch 123/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0379 - mse: 0.0379\n",
            "Epoch 00123: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.3083 - val_mse: 0.3083\n",
            "Epoch 124/160\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0325 - mse: 0.0325\n",
            "Epoch 00124: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0325 - mse: 0.0325 - val_loss: 0.4174 - val_mse: 0.4174\n",
            "Epoch 125/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0323 - mse: 0.0323\n",
            "Epoch 00125: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.3837 - val_mse: 0.3837\n",
            "Epoch 126/160\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0346 - mse: 0.0346\n",
            "Epoch 00126: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.9051 - val_mse: 0.9051\n",
            "Epoch 127/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0328 - mse: 0.0328\n",
            "Epoch 00127: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.3066 - val_mse: 0.3066\n",
            "Epoch 128/160\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0299 - mse: 0.0299\n",
            "Epoch 00128: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.4241 - val_mse: 0.4241\n",
            "Epoch 129/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0303 - mse: 0.0303\n",
            "Epoch 00129: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.3417 - val_mse: 0.3417\n",
            "Epoch 130/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0324 - mse: 0.0324\n",
            "Epoch 00130: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0322 - mse: 0.0322 - val_loss: 0.4031 - val_mse: 0.4031\n",
            "Epoch 131/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0319 - mse: 0.0319\n",
            "Epoch 00131: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.3489 - val_mse: 0.3489\n",
            "Epoch 132/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0298 - mse: 0.0298\n",
            "Epoch 00132: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0301 - mse: 0.0301 - val_loss: 0.3837 - val_mse: 0.3837\n",
            "Epoch 133/160\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0321 - mse: 0.0321\n",
            "Epoch 00133: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0321 - mse: 0.0321 - val_loss: 0.3191 - val_mse: 0.3191\n",
            "Epoch 134/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0292 - mse: 0.0292\n",
            "Epoch 00134: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.3503 - val_mse: 0.3503\n",
            "Epoch 135/160\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0305 - mse: 0.0305\n",
            "Epoch 00135: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0306 - mse: 0.0306 - val_loss: 0.4527 - val_mse: 0.4527\n",
            "Epoch 136/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00136: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.3503 - val_mse: 0.3503\n",
            "Epoch 137/160\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0331 - mse: 0.0331\n",
            "Epoch 00137: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0331 - mse: 0.0331 - val_loss: 0.3457 - val_mse: 0.3457\n",
            "Epoch 138/160\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0271 - mse: 0.0271\n",
            "Epoch 00138: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.3681 - val_mse: 0.3681\n",
            "Epoch 139/160\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0311 - mse: 0.0311\n",
            "Epoch 00139: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0310 - mse: 0.0310 - val_loss: 0.3144 - val_mse: 0.3144\n",
            "Epoch 140/160\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0283 - mse: 0.0283\n",
            "Epoch 00140: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.3938 - val_mse: 0.3938\n",
            "Epoch 141/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00141: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0317 - mse: 0.0317 - val_loss: 0.3390 - val_mse: 0.3390\n",
            "Epoch 142/160\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0301 - mse: 0.0301\n",
            "Epoch 00142: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.3418 - val_mse: 0.3418\n",
            "Epoch 143/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0334 - mse: 0.0334\n",
            "Epoch 00143: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0338 - mse: 0.0338 - val_loss: 0.3373 - val_mse: 0.3373\n",
            "Epoch 144/160\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0276 - mse: 0.0276\n",
            "Epoch 00144: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.3376 - val_mse: 0.3376\n",
            "Epoch 145/160\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0284 - mse: 0.0284\n",
            "Epoch 00145: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.3584 - val_mse: 0.3584\n",
            "Epoch 146/160\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0286 - mse: 0.0286\n",
            "Epoch 00146: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.4200 - val_mse: 0.4200\n",
            "Epoch 147/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0284 - mse: 0.0284\n",
            "Epoch 00147: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.3611 - val_mse: 0.3611\n",
            "Epoch 148/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0292 - mse: 0.0292\n",
            "Epoch 00148: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.3277 - val_mse: 0.3277\n",
            "Epoch 149/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0268 - mse: 0.0268\n",
            "Epoch 00149: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.3580 - val_mse: 0.3580\n",
            "Epoch 150/160\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0264 - mse: 0.0264\n",
            "Epoch 00150: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.3649 - val_mse: 0.3649\n",
            "Epoch 151/160\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0284 - mse: 0.0284\n",
            "Epoch 00151: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.4510 - val_mse: 0.4510\n",
            "Epoch 152/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0253 - mse: 0.0253\n",
            "Epoch 00152: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.3194 - val_mse: 0.3194\n",
            "Epoch 153/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0293 - mse: 0.0293\n",
            "Epoch 00153: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.3661 - val_mse: 0.3661\n",
            "Epoch 154/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0273 - mse: 0.0273\n",
            "Epoch 00154: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.3939 - val_mse: 0.3939\n",
            "Epoch 155/160\n",
            "89/93 [===========================>..] - ETA: 0s - loss: 0.0276 - mse: 0.0276\n",
            "Epoch 00155: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.3335 - val_mse: 0.3335\n",
            "Epoch 156/160\n",
            "90/93 [============================>.] - ETA: 0s - loss: 0.0268 - mse: 0.0268\n",
            "Epoch 00156: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.3865 - val_mse: 0.3865\n",
            "Epoch 157/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0267 - mse: 0.0267\n",
            "Epoch 00157: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 11ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.3777 - val_mse: 0.3777\n",
            "Epoch 158/160\n",
            "88/93 [===========================>..] - ETA: 0s - loss: 0.0287 - mse: 0.0287\n",
            "Epoch 00158: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.4313 - val_mse: 0.4313\n",
            "Epoch 159/160\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0290 - mse: 0.0290\n",
            "Epoch 00159: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.4470 - val_mse: 0.4470\n",
            "Epoch 160/160\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0287 - mse: 0.0287\n",
            "Epoch 00160: val_loss did not improve from 0.03140\n",
            "93/93 [==============================] - 1s 10ms/step - loss: 0.0284 - mse: 0.0284 - val_loss: 0.4090 - val_mse: 0.4090\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "qAr_K2DaK4eu",
        "outputId": "3bc05b61-40d8-45e7-a75d-64cc4b09758c"
      },
      "source": [
        "# plot training and validation accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['mse'])\n",
        "plt.plot(history.history['val_mse'])\n",
        "plt.ylim([0,1])\n",
        "plt.ylabel('Mean Squarred Error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.savefig(\"RegressorCNN2.png\", dpi=300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPYPVBSkYGRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "969e82dc-4521-4f41-a532-08b54446e0d3"
      },
      "source": [
        "train_mseReg=history.history['mse']\n",
        "val_mseReg=history.history['val_mse']\n",
        "test_scoresReg=regressor_model.evaluate(xTest, yTestN, verbose=2)\n",
        "print(\"MSE on Train set:\", train_mseReg[153-1])\n",
        "print(\"MSE on Validation set:\", val_mseReg[153-1])\n",
        "print(\"MSE on Test set::\", test_scoresReg[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20/20 - 0s - loss: 0.4839 - mse: 0.4839\n",
            "MSE on Train set: 0.028874864801764488\n",
            "MSE on Validation set: 0.36613020300865173\n",
            "MSE on Test set:: 0.4839111268520355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzfDm-X_dPN_"
      },
      "source": [
        "## Analysis:\n",
        "1. I have tried using sigmoid and linear function as activation but it did not perform well as the network is being trained to output a floating point value which ranges between -1 to 1.So, tanh is used as it allows the outputs to stay in range (-1,1).\n",
        "2. Neither adam nor sgd worked on this  model but RMSprop with learning rate of 0.00001 reduced the mse of train to 0.0320 which was reduced further with further training of the model with more epochs. \n",
        "3. As it can be seen that the mse for both valid and test data did not decrease from 0.031. From learning curve, it is visible that the mean squarred errro did not change remarkably which could be due to the outputs of the model having values of negative floating points. Also, label encoder was not used as the model is expected to predict negative floating point values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHiAKUdKvTCA"
      },
      "source": [
        "# Section 5: Transfer Learning with VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIlZ-wNkNs9e"
      },
      "source": [
        "Preparing data for VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "GfQFYyy0hpGB",
        "outputId": "979d2a9c-22e1-4947-f770-0d5d5213bfde"
      },
      "source": [
        "import os\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.util import crop\n",
        "path =\"/content/drive/My Drive/Synthetic_Leopard_Circle\"\n",
        "imageDirectory=os.listdir(path) \n",
        "\n",
        "allImg_listN=[]\n",
        "depth=[]\n",
        "\n",
        "for img in imageDirectory:\n",
        "  if \"col\" in img:\n",
        "    image=(io.imread(os.path.join(path,img))) \n",
        "    imgCr=crop(image, ((50, 100), (50, 100), (0,0)), copy=False)\n",
        "    img_gray = rgb2gray(imgCr)\n",
        "    img_resized = resize(img_gray, (80, 50))\n",
        "    allImg_listN.append(img_resized) \n",
        "  else:\n",
        "        depth.append(\"depth File not needed\")\n",
        "\n",
        "print(plt.imshow(allImg_listN[0]))\n",
        "print(allImg_listN[0].shape)\n",
        "\n",
        "#converting the imagelist to numpy array\n",
        "allImg_listN=np.array(allImg_listN)\n",
        "print(allImg_listN.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AxesImage(54,36;334.8x217.44)\n",
            "(80, 50)\n",
            "120\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8YQ7hcGyMfI"
      },
      "source": [
        "### Creating image pairs width depth 3 from grayscale images by adding a blank channel "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muyqscyqOOCj"
      },
      "source": [
        "from numpy import array, dstack\n",
        "#defining data split ranges into Train(0-90), Valid(90-100), Test(100-120)\n",
        "import numpy as np\n",
        "splitTrain=list(range(90))\n",
        "splitValid=list(range(90, 100))\n",
        "splitTest=list(range(100,120))\n",
        "\n",
        "#initializing train, valid and test image data and label lists\n",
        "image_pairsTrainM=[]\n",
        "image_pairsValidM=[]\n",
        "image_pairsTestM=[]\n",
        "angle_DiffTrain=[]\n",
        "angle_DiffValid=[]\n",
        "angle_DiffTest=[]\n",
        "\n",
        "#initializing a blank array of 0 to add a blank channel in the image pair\n",
        "blank_channel=array([0] * 80 * 50) \n",
        "blank_channel= blank_channel.reshape(80, 50)\n",
        "\n",
        "for i in range (0,allImg_listN.shape[0]):\n",
        "    \n",
        "    for j in range(-15,16):\n",
        "      image=allImg_listN[i]\n",
        "      nextImage=allImg_listN[(i+j)%allImg_listN.shape[0]]\n",
        "      img=np.dstack((image, nextImage, blank_channel ))\n",
        "      angularDif=j*3\n",
        "      if i in splitTrain:\n",
        "        image_pairsTrainM.append(img)\n",
        "        angle_DiffTrain.append(angularDif)\n",
        "      \n",
        "      elif i in splitValid:\n",
        "        image_pairsValidM.append(img)\n",
        "        angle_DiffValid.append(angularDif)\n",
        "      \n",
        "      else:\n",
        "        image_pairsTestM.append(img)\n",
        "        angle_DiffTest.append(angularDif)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezTm83zBQFI0",
        "outputId": "6b551467-23dc-48a1-bb80-5dafd0e3c5f5"
      },
      "source": [
        "#converting Train data into numpy array\n",
        "xTrainN=np.array(image_pairsTrainM)\n",
        "print(\"size of image pairs:\", xTrainN.shape)\n",
        "xValidN=np.array(image_pairsValidM)\n",
        "xTestN=np.array(image_pairsTestM)\n",
        "\n",
        "print(\"Total train pairs for train split with start position 90:\",len(xTrainN))\n",
        "print(\"Total valid pairs for valid split with start position 10:\",len(xValidN))\n",
        "print(\"Total test pairs for test split with start position 20:\",len(xTestN))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size of image pairs: (2790, 80, 50, 3)\n",
            "Total train pairs for train split with start position 90: 2790\n",
            "Total valid pairs for valid split with start position 10: 310\n",
            "Total test pairs for test split with start position 20: 620\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHjnLLkZjDAI",
        "outputId": "a6d94597-3dfe-48ad-b836-9c9b3bcccfd9"
      },
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.applications import vgg16\n",
        "# Init the VGG model\n",
        "vgg16_model = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(80, 50, 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfH435OVh5H1",
        "outputId": "015dc63a-1fd0-4d92-ff62-fcbe00c564a1"
      },
      "source": [
        "from keras import applications \n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "# Freeze all the layers\n",
        "for layer in vgg16_model.layers[:-2]:\n",
        "\n",
        "   layer.trainable = False\n",
        "\n",
        "# Create the model\n",
        "vgg_model = Sequential()\n",
        "# Add the vgg convolutional base model\n",
        "vgg_model.add(vgg16_model)\n",
        "\n",
        "\n",
        "vgg_model.add(Conv2D(128, (1, 1), activation='relu', padding='same'))\n",
        "vgg_model.add(BatchNormalization())\n",
        "vgg_model.add(MaxPooling2D((2, 2), padding='same'))\n",
        "\n",
        "# flatten features\n",
        "vgg_model.add(Flatten())\n",
        "vgg_model.add(Dense(1024, activation=tensorflow.keras.layers.LeakyReLU(alpha=0.3))) \n",
        "vgg_model.add(BatchNormalization())\n",
        "vgg_model.add(Dense(1, activation='tanh'))\n",
        "# Show a summary of the model. \n",
        "vgg_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 2, 1, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 2, 1, 128)         65664     \n",
            "_________________________________________________________________\n",
            "batch_normalization_58 (Batc (None, 2, 1, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 1, 1, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_40 (Flatten)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_78 (Dense)             (None, 1024)              132096    \n",
            "_________________________________________________________________\n",
            "batch_normalization_59 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_79 (Dense)             (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 14,918,081\n",
            "Trainable params: 201,089\n",
            "Non-trainable params: 14,716,992\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72BOUsLugvqf"
      },
      "source": [
        "#Creating dataGenerator \n",
        "import tensorflow as tf\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVLPlt0nhK_y"
      },
      "source": [
        "#parameter change optimizer\n",
        "# use early stopping to optimally terminate training through callbacks\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=54)\n",
        "\n",
        "# save best model automatically\n",
        "mc= ModelCheckpoint('yourdirectory/VGG_model25M.h5', monitor='val_loss', \n",
        "                    mode='min', verbose=1, save_best_only=True)\n",
        "cb_list=[es,mc]\n",
        "\n",
        "from keras import optimizers\n",
        "optimizer_new=optimizers.RMSprop(lr=0.0001)\n",
        "# compile model \n",
        "vgg_model.compile(optimizer=optimizer_new, loss='mse', \n",
        "                 metrics=['mse'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUhaUq1fhwmh",
        "outputId": "93f46fc8-4356-4925-9d74-e50a0b50c14b"
      },
      "source": [
        "#this was run with no conv layer added without data generator added The output of basic model\n",
        "history = vgg_model.fit_generator(\n",
        "        datagen.flow(xTrainN, yTrainN, batch_size=30), \n",
        "        epochs=150,\n",
        "        validation_data=val_datagen.flow(xValidN, yValidN, batch_size=10),\n",
        "        callbacks=cb_list)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00001: val_loss improved from inf to 0.03417, saving model to yourdirectory/VGG_model25M.h5\n",
            "93/93 [==============================] - 2s 25ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0342 - val_mse: 0.0342\n",
            "Epoch 2/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00002: val_loss did not improve from 0.03417\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.3466 - val_mse: 0.3466\n",
            "Epoch 3/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00003: val_loss improved from 0.03417 to 0.03274, saving model to yourdirectory/VGG_model25M.h5\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0327 - val_mse: 0.0327\n",
            "Epoch 4/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00004: val_loss did not improve from 0.03274\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.3581 - val_mse: 0.3581\n",
            "Epoch 5/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00005: val_loss did not improve from 0.03274\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0767 - val_mse: 0.0767\n",
            "Epoch 6/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00006: val_loss did not improve from 0.03274\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0419 - val_mse: 0.0419\n",
            "Epoch 7/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00007: val_loss improved from 0.03274 to 0.03155, saving model to yourdirectory/VGG_model25M.h5\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0316 - val_mse: 0.0316\n",
            "Epoch 8/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00008: val_loss improved from 0.03155 to 0.03126, saving model to yourdirectory/VGG_model25M.h5\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0313 - val_mse: 0.0313\n",
            "Epoch 9/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00009: val_loss did not improve from 0.03126\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0315 - val_mse: 0.0315\n",
            "Epoch 10/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00010: val_loss improved from 0.03126 to 0.03122, saving model to yourdirectory/VGG_model25M.h5\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0312 - val_mse: 0.0312\n",
            "Epoch 11/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00011: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0377 - val_mse: 0.0377\n",
            "Epoch 12/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00012: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0587 - val_mse: 0.0587\n",
            "Epoch 13/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00013: val_loss improved from 0.03122 to 0.03122, saving model to yourdirectory/VGG_model25M.h5\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0312 - val_mse: 0.0312\n",
            "Epoch 14/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00014: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0327 - val_mse: 0.0327\n",
            "Epoch 15/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00015: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0337 - val_mse: 0.0337\n",
            "Epoch 16/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00016: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0314 - val_mse: 0.0314\n",
            "Epoch 17/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00017: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0317 - val_mse: 0.0317\n",
            "Epoch 18/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00018: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0319 - val_mse: 0.0319\n",
            "Epoch 19/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00019: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0612 - val_mse: 0.0612\n",
            "Epoch 20/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00020: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0639 - val_mse: 0.0639\n",
            "Epoch 21/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00021: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0337 - val_mse: 0.0337\n",
            "Epoch 22/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00022: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0314 - val_mse: 0.0314\n",
            "Epoch 23/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00023: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0327 - val_mse: 0.0327\n",
            "Epoch 24/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00024: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0323 - val_mse: 0.0323\n",
            "Epoch 25/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00025: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0479 - val_mse: 0.0479\n",
            "Epoch 26/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00026: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0452 - val_mse: 0.0452\n",
            "Epoch 27/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00027: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0330 - val_mse: 0.0330\n",
            "Epoch 28/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00028: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0379 - val_mse: 0.0379\n",
            "Epoch 29/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00029: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0384 - val_mse: 0.0384\n",
            "Epoch 30/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00030: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0498 - val_mse: 0.0498\n",
            "Epoch 31/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00031: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0681 - val_mse: 0.0681\n",
            "Epoch 32/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00032: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0391 - val_mse: 0.0391\n",
            "Epoch 33/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00033: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0929 - val_mse: 0.0929\n",
            "Epoch 34/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00034: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0358 - val_mse: 0.0358\n",
            "Epoch 35/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00035: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0490 - val_mse: 0.0490\n",
            "Epoch 36/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00036: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0321 - val_mse: 0.0321\n",
            "Epoch 37/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00037: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0312 - val_mse: 0.0312\n",
            "Epoch 38/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00038: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0312 - val_mse: 0.0312\n",
            "Epoch 39/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00039: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0316 - val_mse: 0.0316\n",
            "Epoch 40/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00040: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0324 - val_mse: 0.0324\n",
            "Epoch 41/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00041: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0334 - val_mse: 0.0334\n",
            "Epoch 42/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00042: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0325 - val_mse: 0.0325\n",
            "Epoch 43/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00043: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0437 - val_mse: 0.0437\n",
            "Epoch 44/150\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00044: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0370 - val_mse: 0.0370\n",
            "Epoch 45/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00045: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0449 - val_mse: 0.0449\n",
            "Epoch 46/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00046: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0337 - val_mse: 0.0337\n",
            "Epoch 47/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00047: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0312 - val_mse: 0.0312\n",
            "Epoch 48/150\n",
            "92/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00048: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0347 - val_mse: 0.0347\n",
            "Epoch 49/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00049: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0362 - val_mse: 0.0362\n",
            "Epoch 50/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00050: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 23ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0332 - val_mse: 0.0332\n",
            "Epoch 51/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00051: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0339 - val_mse: 0.0339\n",
            "Epoch 52/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00052: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0355 - val_mse: 0.0355\n",
            "Epoch 53/150\n",
            "91/93 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00053: val_loss did not improve from 0.03122\n",
            "93/93 [==============================] - 2s 22ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0399 - val_mse: 0.0399\n",
            "Epoch 00053: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "Mubxz5lYRcFu",
        "outputId": "c9e9459b-8392-47f8-acfa-04055b04b49b"
      },
      "source": [
        "# plot training and validation accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['mse'])\n",
        "plt.plot(history.history['val_mse'])\n",
        "plt.ylim([0,0.6])\n",
        "plt.ylabel('Mean Squarred Error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.savefig(\"VGGModel1.png\", dpi=300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZhU5Zn38e/de9M0IIiKYAQFJCoKiGhcEhSduGLGHc0oMRNfHU103hhjTFziktU3iSYmGWISsxgZY6LBcRc1GskoqERFRVExtgoisnQDvd/vH8+ppmiqq043XdVdVb/PddVVdU4959RzmqLu8+zm7oiISPEq6esMiIhI31IgEBEpcgoEIiJFToFARKTIKRCIiBQ5BQIRkSKX1UBgZkeZ2VIzW2Zml3WR5lQze9nMlpjZH7KZHxER2ZplaxyBmZUCrwFHAnXAQmCWu7+clGYccAdwuLuvMbMd3P2DrGRIRERSymaJYBqwzN3fdPdmYC5wQqc0XwBudvc1AAoCIiK5V5bFc48E3knargMO6JRmPICZPQWUAle7+wOdT2Rm5wLnAtTU1Ow3YcKErGRYRKRQPfvssx+6+/BU72UzEMRRBowDpgOjgCfMbKK7r01O5O5zgDkAU6dO9UWLFuU6nyIiec3M3u7qvWxWDb0L7JK0PSral6wOmOfuLe7+FqFNYVwW8yQiIp1kMxAsBMaZ2RgzqwBOB+Z1SnM3oTSAmW1PqCp6M4t5EhGRTrIWCNy9FbgQeBB4BbjD3ZeY2TVmNjNK9iCw2sxeBh4DvuLuq7OVJxER2VrWuo9mS6o2gpaWFurq6mhsbOyjXBWeqqoqRo0aRXl5eV9nRUR6gZk96+5TU73X143FvaKuro7a2lpGjx6NmfV1dvKeu7N69Wrq6uoYM2ZMX2dHRLKsIKaYaGxsZNiwYQoCvcTMGDZsmEpYIkWiIAIBoCDQy/T3FCkeBRMIRESkZxQIesHq1auZNGkSkyZNYqeddmLkyJEd283NzWmPXbRoEV/60pdylFMRka0VRGNxXxs2bBiLFy8G4Oqrr2bgwIFccsklHe+3trZSVpb6Tz116lSmTk3ZkC8ikhMqEWTJ7NmzOe+88zjggAO49NJLeeaZZ/jEJz7B5MmTOeigg1i6dCkAjz/+OMcddxwQgsg555zD9OnT2W233bjpppv68hJEpEgUXIngm/cs4eX31vfqOffceRBXHb9Xt4+rq6tjwYIFlJaWsn79ep588knKysp45JFHuPzyy/nTn/601TGvvvoqjz32GPX19eyxxx6cf/756ssvIllVcIGgPznllFMoLS0FYN26dZx99tm8/vrrmBktLS0pjzn22GOprKyksrKSHXbYgZUrVzJq1KhcZltEikzBBYKe3LlnS01NTcfrK664gsMOO4y77rqL5cuXM3369JTHVFZWdrwuLS2ltbU129kUkSKnNoIcWbduHSNHjgTg1ltv7dvMiIgkUSDIkUsvvZSvfe1rTJ48WXf5ItKvFMSkc6+88gof//jH+yhHhUt/V5HCkW7SOZUIRESKnAKBiEiRUyAQESlyCgQiIkVOgUBEpMgpEIiIFDkFgl5w2GGH8eCDD26x70c/+hHnn39+yvTTp08n0QX2mGOOYe3atVulufrqq7nhhhvSfu7dd9/Nyy+/3LF95ZVX8sgjj3Q3+yJS5BQIesGsWbOYO3fuFvvmzp3LrFmzMh573333MWTIkB59budAcM0113DEEUf06FwiUrwUCHrBySefzL333tuxCM3y5ct57733uP3225k6dSp77bUXV111VcpjR48ezYcffgjA9ddfz/jx4znkkEM6pqkG+MUvfsH+++/Pvvvuy0knncTGjRtZsGAB8+bN4ytf+QqTJk3ijTfeYPbs2dx5550AzJ8/n8mTJzNx4kTOOeccmpqaOj7vqquuYsqUKUycOJFXX301m38aEckDBTfpHPdfBite7N1z7jQRjv5Ol28PHTqUadOmcf/993PCCScwd+5cTj31VC6//HKGDh1KW1sbM2bM4IUXXmCfffZJeY5nn32WuXPnsnjxYlpbW5kyZQr77bcfACeeeCJf+MIXAPjGN77BL3/5S774xS8yc+ZMjjvuOE4++eQtztXY2Mjs2bOZP38+48eP56yzzuJnP/sZF198MQDbb789zz33HD/96U+54YYbuOWWW3rjryQieUolgl6SXD2UqBa64447mDJlCpMnT2bJkiVbVON09uSTT/Kv//qvDBgwgEGDBjFz5syO91566SUOPfRQJk6cyG233caSJUvS5mXp0qWMGTOG8ePHA3D22WfzxBNPdLx/4oknArDffvuxfPnynl6yiBSIwisRpLlzz6YTTjiB//zP/+S5555j48aNDB06lBtuuIGFCxey3XbbMXv2bBobG3t07tmzZ3P33Xez7777cuutt/L4449vU14TU11rmmsRAZUIes3AgQM57LDDOOecc5g1axbr16+npqaGwYMHs3LlSu6///60x3/yk5/k7rvvZtOmTdTX13PPPfd0vFdfX8+IESNoaWnhtttu69hfW1tLfX39VufaY489WL58OcuWLQPgd7/7HZ/61Kd66UpFpNAoEPSiWbNm8Y9//INZs2ax7777MnnyZCZMmMAZZ5zBwQcfnPbYKVOmcNppp7Hvvvty9NFHs//++3e8d+2113LAAQdw8MEHM2HChI79p59+Ot///veZPHkyb7zxRsf+qqoqfv3rX3PKKacwceJESkpKOO+883r/gkWkIGR1GmozOwq4ESgFbnH373R6fzbwfeDdaNdP3D1ty6Wmoc4d/V1FCke6aaiz1kZgZqXAzcCRQB2w0MzmuXvnFtP/dvcLs5UPERFJL5tVQ9OAZe7+prs3A3OBE7L4eSIi0gPZDAQjgXeStuuifZ2dZGYvmNmdZrZLTz8s31Za6+/09xQpHn3dWHwPMNrd9wEeBn6TKpGZnWtmi8xs0apVq7Z6v6qqitWrV+vHq5e4O6tXr6aqqqqvsyIiOZDNcQTvAsl3+KPY3CgMgLuvTtq8BfheqhO5+xxgDoTG4s7vjxo1irq6OlIFCemZqqoqRo0a1dfZEJEcyGYgWAiMM7MxhABwOnBGcgIzG+Hu70ebM4FXevJB5eXljBkzZlvyKiJStLIWCNy91cwuBB4kdB/9lbsvMbNrgEXuPg/4kpnNBFqBj4DZ2cqPiIikltVxBNmQahyBiIikl24cQV83FouISB9TIBARKXIKBCIiRU6BQESkyCkQiIgUOQUCEZEip0AgIlLkFAhERIqcAoGISJFTIBARKXIKBCIiRU6BQESkyCkQiIgUOQUCEZEip0AgIlLkFAhERIqcAoGISJFLGwjMrMTMDspVZkREJPfSBgJ3bwduzlFeRESkD8SpGppvZieZmWU9NyIiknNxAsH/Af4INJvZejOrN7P1Wc6XiIjkSFmmBO5em4uMiIhI38gYCADMbCbwyWjzcXf/n+xlSUREcilj1ZCZfQe4CHg5elxkZt/OdsZERCQ34pQIjgEmRT2IMLPfAM8DX8tmxkREJDfiDigbkvR6cDYyIiIifSNOieBbwPNm9hhghLaCy7KaKxERyZm0gcDMSoB24EBg/2j3V919RbYzJiIiuRFnZPGl7v6+u8+LHrGDgJkdZWZLzWyZmXVZiogGrLmZTe1G3kVEpBfEaSN4xMwuMbNdzGxo4pHpIDMrJUxPcTSwJzDLzPZMka6W0Cvp6W7mXUREekGcNoLToucLkvY5sFuG46YBy9z9TQAzmwucQOiCmuxa4LvAV2LkRUREelnG2UeBy9x9TKdHpiAAMBJ4J2m7LtqXfP4pwC7ufm+GfJxrZovMbNGqVatifLSIiMQVp40gK3fqUZD5AfDlTGndfY67T3X3qcOHD89GdkREilbW2giAd4FdkrZHRfsSaoG9gcfNbDmhZ9I8NRiLiORWNtsIFgLjzGwMIQCcDpzRcQL3dcD2iW0zexy4xN0XxciTiIj0kjizj47pyYndvdXMLgQeBEqBX7n7EjO7Bljk7vN6cl4REeldXVYNmdmlSa9P6fTet+Kc3N3vc/fx7r67u18f7bsyVRBw9+kqDYiI5F66NoLTk153nmDuqCzkRURE+kC6QGBdvE61LSIieSpdIPAuXqfaFhGRPJWusXjfaG1iA6qT1ik2oCrrORMRkZzoMhC4e2kuMyIiIn0j7sI0IiJSoBQIRESKnAKBiEiRUyDorr/9EJbN7+tciIj0mi4bi82snjTdRN19UFZy1N89dSOM+zSMndHXORER6RXpeg3VApjZtcD7wO8IXUfPBEbkJHf9UfMGaKrv61yIiPSaOFVDM939p+5e7+7r3f1nhJXGik9rM7Q1Q9P6zGlFRPJEnECwwczONLNSMysxszOBDdnOWM588Aq8el+8tM0N4blxXfbyIyKSY3ECwRnAqcDK6HEKSesK5L0FP4F7LoqXNhEIVDUkIgUkznoEyynkqqCmdfGrepoSgUBVQyJSODKWCMxsvJnNN7OXou19zOwb2c9ajjQ1QGsjtLVmTttRNbQeXPPuiUhhiFM19AvCegQtAO7+AluuVZDfEj/uzTGqexJVQu0tIXiIiBSAOIFggLs/02lfjNvnPNFR3dOQOW1zUhu52glEpEDECQQfmtnuRIPLzOxkwriCwpD4QW+OEwiS0jSqnUBECkPGxmLgAmAOMMHM3gXeIgwqKwyJKqE4JYLkNE3qQioihSFtIDCzUuA/3P0IM6sBSty9cOpE3LvXEyi5RKCqIREpEGkDgbu3mdkh0evCGUSW0NoI3hZeq2pIRIpUnKqh581sHvBHkkYUu/ufs5arXEm+q+921ZACgYgUhjiBoApYDRyetM+BwgoEcUsE5TXQoonnRKRwxGkjWO3ul+QoP7nV3Tr/5gYYNAJWL1PVkIgUjLTdR929DTg4R3nJvaZuBoKmBqgaDOUDVDUkIgUjTtXQ4qJoI4hbNVQxECoHKRCISMGIM6AsuY3g+OhxXJyTm9lRZrbUzJaZ2WUp3j/PzF40s8Vm9jcz27M7md9mW1QNxRxZXFkLVYNUNSQiBSPO7KOf68mJo/aFm4EjgTpgoZnNc/eXk5L9wd1/HqWfCfwAOKonn9cjiRJB9dB4JYKmeqioCcFAJQIRKRAZA4GZVQGfB/YilA4AcPdzMhw6DVjm7m9G55lLmM66IxC4e/KvaQ1p1kjOisSPf+2I+I3FHVVD6jUkIoUhTtXQ74CdgE8DfwVGAXF+BUcC7yRt10X7tmBmF5jZG8D3gC+lOpGZnWtmi8xs0apVq2J8dEyJH/OBO8QsETRA5UBVDYlIQYkTCMa6+xXABnf/DXAscEBvZcDdb3b33YGvAinXOXD3Oe4+1d2nDh8+vLc+OvywV0Q/7Jnu8NtaoK0JKmpVNSQiBSVOIGiJntea2d7AYGCHGMe9C+yStD0q2teVucBnYpy39zTXR1U9tZkbixMlhooaqBysEoGIFIw4gWCOmW0HXAHMI9Txfy/GcQuBcWY2xswqCIvZzEtOYGbjkjaPBV6PlevekqjqqajNXDWUCBSJqqGWDdDelv08iohkWZxeQ7dEL/8K7Bb3xO7eamYXAg8CpcCv3H2JmV0DLHL3ecCFZnYEodSxBji7uxewTToafweG1+5g1kXaaAhFogQBoXqoervc5FVEJEvi9Bq6MtV+d78m07Hufh9wX6d9Vya9vihGHrOnqT78qFcMBG+Hlo2h6ieVjqqhqNcQhOohBQIRyXNxqoY2JD3agKOB0VnMU+40NYRAUDlw83aXaaPG5ETVUPI+EZE8Fqdq6P8lb5vZDYTqnvyXaCyuSFT11EPtjl2kTSoRtDZF6dVgLCL5L85cQ50NIPQAyn+JxuJEnX9zmjv8jjaCmtCVFNRzSEQKQpw2ghfZPOK3FBgOZGwfyAuJNoJuVQ3VhvaE5H0iInksTokgeYK5VmClu7dmKT+5kzxArCIKBOm6kCZXDXkUF7WAvYgUgDiBoPNt7yBL6mLp7h/1ao5yJbnxt6M7aLpAsAGsBMqrN+9T1ZCIFIA4geA5wgjhNYABQ4B/Ru853Rhb0K8k3+F3lAjSVPUkpqMwC8GgpExVQyJSEOJ0H30YON7dt3f3YYSqoofcfYy752cQgC5KBOkai+s3BwwzLU4jIgUjTiA4MBoYBoC73w8clL0s5UjHlBG10SAyy9BY3LC5UTlxnKqGRKQAxKkaes/MvgH8Pto+E3gve1nKkUQ1UEVtuMOvGJihsXjDlqOO48xYKiKSB+KUCGYRuozeFT12iPblt+RJ5BLPaauGGjZXDUGYgVRVQyJSAOKMLP4IuAggmoV0rbvndiWxbEj86Cd+3DOVCJoaYEjSrNqVtbCuLnv5ExHJkS5LBGZ2pZlNiF5XmtmjwDJgZTRjaH5rTmojgKhEkGEcwVZVQxpHICL5L13V0GnA0uj12VHaHYBPAd/Kcr6yr6lzIKjtZtWQ2ghEpDCkCwTNSVVAnwZud/c2d3+Fns1R1L8010NpJZSWh+1Mi9N07jWUWLe4AGrJRKS4pQsETWa2t5kNBw4DHkp6b0B2s5UDTfWduoOmaSxua4XWTZtnKYVozqG2sIaBiEgeS3dnfxFwJ6HH0A/d/S0AMzsGeD4HecuuxFoECekai1uSZh5NqExak6CrxWxERPJAl4HA3Z8GJqTYv9WqY3mpuaHTHX6axuLOXU0BqgaH58b1ULtTdvIoIpIDccYRFKbOVUMVtWE20tbmrdMmz0uUkLxusYhIHivuQJDqhz1V9VDKQJCoGlIgEJH8VryBoLlTG0HH4jQpGoxTVg0lLWAvIpLHYnUDNbODCAvWd6R3999mKU+50bk7aLrFaVQ1JCIFLM5Slb8DdgcWA23RbgfyOxCkaiyG1A3GTemqhjSoTETyW5wSwVRgz4KYXyihvT2qGurUWAypF6dpTlE1lCgRqGpIRPJcnDaCl4DC6h+ZsqonTRtBqvQlpWFbVUMikufilAi2B142s2eApsROd5+ZtVxlW+cJ55Jfd1k1ZFDeaUC1VikTkQIQJxBcne1M5FznCecgQ2NxtChNSacCVGK+IRGRPBZnPYK/9vTkZnYUcCNQCtzi7t/p9P7/Bf4daAVWAee4+9s9/bzYmjutRQDpSwTNncYcdByjEoGI5L+MbQRmdqCZLTSzBjNrNrM2M8v462dmpcDNwNHAnsAsM9uzU7Lnganuvg9hXqPvdf8SeiB54fqE0vIwG2mqxuLOXU0TtG6xiBSAOI3FPyEsTfk6UE24g785xnHTgGXu/qa7NwNzgROSE7j7Y+6emL7zf4FRcTO+TVJVDUHX8w11XpQmQesWi0gBiDWy2N2XAaXRegS/Bo6KcdhI4J2k7bpoX1c+D9yf6g0zO9fMFpnZolWrVsXJcnqpegEltlP2Gtqw5ZiDBFUNiUgBiNNYvNHMKoDFZvY94H16eWoKM/ssYbzCp1K97+5zgDkAU6dO3fbxDB1VQ51LBINSNxY31cOgnbfer6ohESkAcX7Q/y1KdyGwAdgFOCnGce9GaRNGRfu2EK1//HVgprs3dX4/KzovXJ/Q1eI0XVYNDQ4L1rS19H4eRURyJE6vobfNrBoY4e7f7Ma5FwLjzGwMIQCcDpyRnMDMJgP/BRzl7h9049zbprkBrBTKq7fcXzEQNn6YIv2GrnsNQQgeA4b2fj5FRHIgTq+h4wnzDD0QbU8ys3mZjnP3VkIp4kHgFeAOd19iZteYWWIw2veBgcAfzWxxnPP2ikQvILMt93fVWNx5NbOO9IlpJtb1fh5FRHIk7oCyacDjAO6+OLrLzyjVambufmXS6yPiZrRXdZ5wLiHVcpXt7WGpylQlgipNPCci+S9OG0GLu3e+5c3vCeia1ncxLiBFd9COHkYp2gi0OI2IFIA4gWCJmZ0BlJrZODP7MbAgy/nKrqaGLur8oxJBe/vmfc0bNr+3VXrNQCoi+S9OIPgisBdhwrnbgfXAxdnMVNZ1Xp0sIREcWjZsmRZSVyUlFrBX1ZCI5LE4vYY2Erp3fj372cmRpgaoTTGzdvLiNB1zDyW6mqpqSEQKU5eBIFMPnryehrqpvovG4hQL2KerGupYt1i9hkQkf6UrEXyCMEXE7cDTgKVJm1+a67uo80+xOE1X01EAlFVCaYWqhkQkr6ULBDsBRxImnDsDuBe43d2X5CJjWeOeeVxA8g97VxPUdRyj+YZEJL912VgcTTD3gLufDRwILAMeN7MLc5a7bGhtBG9LfYefanGa5jRtBKDFaUQk76VtLDazSuBYQqlgNHATcFf2s5VFXU04l7yvKUUbQarAkThGJQIRyWPpGot/C+xNGBn8TXd/KWe5yqauJpxL3tecomqoqxJBqkFoIiJ5JN04gs8C44CLgAVmtj561MdZoazfSrVwfUJy99Hk9OUDoKQ09fmqBqtqSETyWpclAnfv1TUH+o2Oxt8UJYLyAWAlndoIuhiFnKCqIRHJc4X5Y59OupHCZmF/515DqYJGgnoNiUieK75AkGrh+mSdp6LualGahMS6xZ7f8/CJSPEq3kDQVXVPxcCtG4tTlR4SKmvB21MvcSkikgeKLxCkayyG1CWCTFVDoJ5DIpK3ii8QNKWZMiKxvzuNxR3zDamdQETyUxEGgnoor4GSLi69MkVjcbo2gsrEVNQKBCKSn4ovEHQ14VxCZe3WI4u7qkZKpAcFAhHJW8UXCLqacC4hubHYXVVDIlLwii8QZBwgltRY3LwB8AxVQ1qcRkTyW/EFgqb6zCWC9hZobUrqYZShKilxXhGRPFSEgSDGlBGJdB0zj2YIHJiqhkQkbxVfIGiOUSKAUNWTbr3ihJISTTMhInmt+AJBxrmDktYtjlM1lDhGJQIRyVNFGAjqMzcWQwgYTWkmqEtWpRKBiOSv4goEbS3Q1pShaqgnJQIFAhHJX1kNBGZ2lJktNbNlZnZZivc/aWbPmVmrmZ2czbwA6ZepTOgoEdQnTVmdpo0AtG6xiOS1rAUCMysFbgaOBvYEZpnZnp2S/ROYDfwhW/nYQnOGeYaS32tuyDwvUULnaSlERPJI2sXrt9E0YJm7vwlgZnOBE4CXEwncfXn0XnsW87FZprUIYMtxAZkWru84RlVDIpK/slk1NBJ4J2m7LtrXd+I0/lYkNRY310NZFZRmiJeqGhKRPJYXjcVmdq6ZLTKzRatWrer5iZpjlAhKy6CsOqTNNPgsobI2NEK3NvU8byIifSSbgeBdYJek7VHRvm5z9znuPtXdpw4fPrznOWrKsChNQmK+oUyL0nSkT0xFrXYC6Qdam+Gxb0HDNtw0SVHJZiBYCIwzszFmVgGcDszL4udllmmZyoTE4jTNG+KVCDpmIF23bfkT6Q3LHoG/fhee+01f50TyRNYCgbu3AhcCDwKvAHe4+xIzu8bMZgKY2f5mVgecAvyXmS3JVn6AzMtUJiRKBJkGn3Wk18Rz0o+8dn94XvZI3+ZD8kY2ew3h7vcB93Xad2XS64WEKqPciN0ddFD4UW/ZAAOGZT6vpqKW/qK9HV57CDB45xnYtBaqh/R1rqSfy4vG4l7TXA+llVBWkT5dYnGablcNKRBIH3t/MTSsgClngbfBW3/t6xxJHiiuQJBpwrmEjqqhbvQaApUIpO+99gBgcNjXQyeG1x/u6xxJHshq1VC/E7fOP9FY3LKpe72GNq3dtvyJbKul98Mu06B2R9h9OiybH5ZcNevrnEk/VlwlguYM6xUnJKaMyLSsZUL1djD4Y/BK33aKkiK37l1Y8QKMPypsjz0C6t+DD15Of5wUveIKBJmWqUyoGAgtG8HbM084B2Fxmk9cAP/8O/zz6W3Pp0hPvP5geN7j6PA89ojwrN5DkkFxBYK4d/jJwSJO4ACY8m9QPRSe+lHP8ibSFXdYeAu89WT6dEsfgCG7wvAJYXvQzrDDXgoEklFxBYKm+viNxQlxAgeEksO0c2HpffDBqz3Ln6TW3g6b1vR1LvrOgh/DvV+GOz/X9ViV5o2hh9AeR2/ZHjB2Brz9d41xkbSKLBDELBEkp4lTNZQw7dwwT9GCm7qfNwk2fBjuYBf8GO7+D5gzHb49Er47OjR8FpsX74SHr4CPHQQbVsFTN6ZO9+bj0NoI4z+95f5xR0J7S+bShBS14goE3Wks7ngds0QAUDMs9N9+4Y7QcNff1K8M1Qz91arX4Ef7wO9Pgoe+EQJC1WCYcnZojH/4qlA6KBZvPQl3nw+7Hgz/dhfsfRIs+Enq79Zr94dZdXc9ZMv9uxwI5TWqHpK0iicQtLfHDwRblAhithEkfOKC0Mj8vz/t3nHZ9ubj8IMJMP+avs5J1x65CqwEzvoLfOVNuOS18Pro78CMK2Hli/DSn/o6l7mx8mWYeyZsNwZOvw3Kq2DGVWGQ2KPXbZk2MZp47OFbD5Ysq4DdPgXLHu7fNwHSp4onEMRZnSwhuRTQnRIBwHa7wt4nwrO39p967fXvwZ2fD68X/Bg+XNa3+Ull+VOhfeWQi2G36aF0lWzvk2CnifDYdWF2zUK2/j247WQor4bP3hm6J0P4bh1wHvzjdnj/H5vTJ0YTjz869fnGHgFr/wmr++G/u/QLxRcIYjUWJ5UCutNGkHDwReHzFv6y+8f2trYW+OPsUH981ryw0M6DX+vrXG2pvT1UBdXuDAf+R+o0JSUw42pYszwE2ULVuA5uOyU8n/lHGPKxLd8/9MshMDz0jc13+InRxOOOTH3OsTPCs0YZSxeKJxDEWZ0sITlN3F5DyXaaGO7Cnv55GJ3clx6+Et55Gmb+GMYcCtO/Cq8/FLoa9hdL/gzvPQczroCKAV2nGzsj1IE/8b3N/57ZtOIlqHs2N58FsPoN+O0JsOpVOO13MGKfrdNUD4Hpl8FbT4R/R9g8mrhm+9Tn3W40DBvXdTvBG4/B/V/dvDSrFJ3iCQQdq5PFaSzuQffRzg6+OPTyWPyHnh3fG5bcFdoqDjgvVFcBTPs/sP14eOCy/rGiWmsTzP8m7DgR9jktfVozOOLq8HfNdhvMol/Dzw+BWw4PvZZu3BdunxXaWF76E7Q09t5nucPzv4efHwofvQmn/hZ2P7zr9Pt9DobuDg9dAWve3nI0cVfGHQlvP7XljYl7aMMbujgAAA9LSURBVHz+/YnhpmXuGb17Xf1Rwwd9f3PWDxVPIIizcH1CWRVYKZRWZJ6ptCujD4GR+4U6+fa2np1jW3z4OvzlQhg1DY68dvP+sgo46juw5i34+09yn6/OnvlFqL/+l2ugpDRz+l32hwnHwVM3ha6m2fD0f8H/XBx+PE/7PRz2Ddh5Cnz0Vui+eec5cMsR4Q5+W238CO44C/5yAYycAucvgAnHpj+mrAKOvAY+XAp/itp+9uiifSBh7IxQPbj8b2G7pTF0z33o6+Hzjv1B6FBwx1n50wbT2gSPXh/yvPSB9P/PVrwEd5wNN4yHm6fB2wtyl888UESBoBuNxWYhYPS0NJA4x8EXhx/cO86C9e/3/Fzd1bwhfGZZJZxy69bBbOyM8GP6xA1928110xp44vuw+4z0d8CdzbgyrBXx5P/r/Tw9dSPcf2n4+5x2G3z8ePjUV+CUX8MF/wuXvxf2r6+D//rktvVievOv8LODQ9XOEd8MPaQGx1yeY8KxYWxB3cLQjpAYTdyVXQ8JY1yWPRK+i7ceC//4A0y/HE75Lez/eTjuh2Gaij//O7S19vy6cmHFS/CLw0M14VtPwO2nwU2T4G8/3PIG4d3n4PYz4OcHh3EoB/5HuMn79TGh2rQ/lIrj+OjNcNP04etZOX3xzD4ad3WyhMpB2z5j48ePD//BH/823HxAuOudfFZo+MyW1W+EKoMPXoF/+zMMHpk63b9cF/L0yFVw0i3Zy086T9wQGkWP7GaX1uF7wKQzwrQLB56/dYNqT/31e/DY9bDXiXDiHCgt3zpNWSV8/DjYeVIoGdx5TrjL/vS3QxfPBHdYtTT073/32VAd0doUPRrD84evwbCxMOsPsPPk7uXVDD59Xfgx3OOYzN/V8qpQSn35L7Dk7lBCPu334TuaMPWckM8HL4eyC+AzP8vud7Un2ttCSfbR66BqCMz673Bj8+q94fvwyNVhvea9/hU2ro7GogwJAe+Ac0ND+2GXh5LQUzeG4HDiHNhxr23PW1srtLdu+T3oqaYGWP5kyN8b80MggPA9237ctp+/k+IJBE3daCOAUBrY1kBgFrpDfvx4uOei8Hjhj3D8jbD92G07d7LWZnj1ntCb5q0noKQM/uXa9HfZQ8fAwV8Kd+RTz4FdD0r/GYlxGE3rw4/coJ3jVeV0Zc1yeGYOTDoTdtq7+8dP/1r4W86/Fk64uedVeBCu59Hr4MkbYN9Z4XyZrm3wKJh9Lzx6bfhBqVsIJ94C9e/Daw+GALBmeUg7bGy4sSirCr3QBgzdHFAO/XLPeqZBqHqcfS/ssGe89GOPCOMJhuwabhJS/fh94oIw4eKj14Xuq8f9MLdTWLtDW3Oolu38uWveDgPs3n4qlNiOv3FzA/lenwmPD16FRb+ExbeH78SMq2D/f9+8eBSE0v7xN4butvO+GEavH34FTP1c+t8H9/CD/O6zsOLF0N7QsDI8b/hgc0lk2O7hb7vjxPC8095QOyIE2ZaN0WNTmBZk00dQvyI614rwuv59eP+FMCK8vCZ08jjg/BDwhu7Wq3/uBPM8G2QydepUX7RoUbePW/vQdxmy4FssOO0l2suqM6bf58GTAXjh03d2+7NScmfHN/7ImOe+RUlbE3Uf/3caB43GrZz2kjK8pAy3MrASzNswb4X28GztbRge0lmUtqQctxKGrPg7O7z5JyqaPqKxZhQrxp7Gyt1PoaV6eMYslbRuYso9R9JWMZi6Pb9AxaZVlDeuomLTh1Q0rqK8cTWlzfWUtdRT2tKAsfm70l5SQePAXWis3ZVNtbvSOHBXWqqGhrx6C9beGj1aoutpw7wdouchKxYwcPULPDtzPs0DdurRn3T0c99m1Cu34FZCU81INg3cNcrPaForh1DWvI7yxtWUNa2hPHqUtDXiJeW0l1aE55IKSls3MGTF31kx9lTeOOD6MKitG7are4xxCy6hvHltx99m7U4HsWbUDD4aeRjNNSN6dH29rbSlgZ1eu42VY0+ltXK7tGk/tvgGdnnpZ6wedQQbh4ynrXxgx6O1fCBeUoF3/FBv/sE2b8PaWyhpb8HamqPXzZS2bKS0dQOlLQ2UtmyIXie26ylrrg/PLQ2Yt+FWQltZDW3lNeFzy2qoXh/aZN6ceiWrdjsxbYCytiawErwkRakuSVnjasY+/XWGvRO61jZV78imQWPYNGg3Ng3ajeaaEQxY+zoDP3ye2tUvUN4Uxga1l1TQXL0DLdXb01y1PS1V29NcPRwwBqxdSs2aV6hu+GeGf5EttVQMobl6OC3VO9AwdC/W7nwo64fvh5dWdqTZbXgNIwZn/v1KxcyedfepKd8rlkBw6yPPcfv8p1nqu5D8xe3KUSXPAPBA+7Ruf1Y6w1nD1eW/4djSZ3rlfK1ewiPt+/GHtsN5sn0i3s1mn2NK/pefVmyeG6nRy/nAh7CKIaz2QaynhnqvZj0DqPcBNFCN4XzMPmC0rWC0rWRXW8kA615da5sb32o9k1+2HdOt45KV0coxJc+we8l7jLYV7GorGGMrGGwbO9K0eglrqOUjr+UjH0QT5ZTTSrm1UkF4lNPKQ+37cUPrqd3++yWMYDWnlj7Oy74rf2vfm030QvVAn3IuKbuDz5Y+wkA2UWa9M7XHJq+ggSoavJoNVNNANfU+gPXRcwPVbPJKqq2JGhoZyCZqrJEaGlnPAL7bOos6z3yT0z3OoSUvMtHeYreS99nd3mM3e6/je9Tuxus+ksXtY1nsu/N8+zhe95G0kb7UWMMm9rB32LPkbbajno1U0kglm7yCjVTRSAXrvKbj/1sz6YMWwHWf2ZvPHrhrj65SgQB4f90m6tb0n25jZRtXUdq2Kdwxt4e7/xJvgfb26I6/NJQQEs8Q7rA96U67vZXG2l1pHbBt/zGqP3qF9tJKWqqH017egyoxd8o3fUBp07qO0golpbRbOV6SuIbwwEo6nrNV5VDauIay5rW0Vg6lraJ2izv8PPu69w/ulLQ1hrv36M7d2ls63ktKCFZGe2l5KLFGJS4vKae9fABtZQNCtWU+cKescTUVG9+nsXY07d2daiZLRg8bwA6DenaTkS4Q5Mm/yrYbMbi6x0Wq7Bja1xnYbPTBvXCSYZmT5Ew/+ttKHhsGjO/rTOREP+sSICIiuaZAICJS5BQIRESKnAKBiEiRUyAQESlyCgQiIkUuq4HAzI4ys6VmtszMLkvxfqWZ/Xf0/tNmNjqb+RERka1lLRCYWSlwM3A0sCcwy8w6T4ryeWCNu48Ffgh8N1v5ERGR1LJZIpgGLHP3N929GZgLnNApzQnAb6LXdwIzzHI5w5WIiGRzZPFI4J2k7TrggK7SuHurma0jDOfbYsURMzsXODfabDCzpT3M0/adz13AiuVai+U6oXiutViuE3J7rV1OUpQXU0y4+xxgzraex8wWdTXXRqEplmstluuE4rnWYrlO6D/Xms2qoXeBXZK2R0X7UqYxszJgMLA6i3kSEZFOshkIFgLjzGyMmVUApwPzOqWZB5wdvT4ZeNTzbTpUEZE8l7WqoajO/0LgQaAU+JW7LzGza4BF7j4P+CXwOzNbBnxECBbZtM3VS3mkWK61WK4Tiudai+U6oZ9ca96tRyAiIr1LI4tFRIqcAoGISJErmkCQabqLfGZmvzKzD8zspaR9Q83sYTN7PXpOv1p5HjCzXczsMTN72cyWmNlF0f6CulYzqzKzZ8zsH9F1fjPaPyaaimVZNDVLRV/ntbeYWamZPW9m/xNtF9y1mtlyM3vRzBab2aJoX7/47hZFIIg53UU+uxU4qtO+y4D57j4OmB9t57tW4MvuvidwIHBB9O9YaNfaBBzu7vsCk4CjzOxAwhQsP4ymZFlDmKKlUFwEvJK0XajXepi7T0oaO9AvvrtFEQiIN91F3nL3Jwi9rpIlT9/xG+AzOc1UFrj7++7+XPS6nvDDMZICu1YPGqLN8ujhwOGEqVigAK4zwcxGAccCt0TbRoFeawr94rtbLIEg1XQXI/soL7myo7u/H71eAezYl5npbdFMtZOBpynAa42qShYDHwAPA28Aa929NUpSSN/hHwGXAu3R9jAK81odeMjMno2mzYF+8t3NiykmZNu4u5tZwfQTNrOBwJ+Ai919ffI8hYVyre7eBkwysyHAXcCEPs5SVpjZccAH7v6smU3v6/xk2SHu/q6Z7QA8bGavJr/Zl9/dYikRxJnuotCsNLMRANHzB32cn15hZuWEIHCbu/852l2Q1wrg7muBx4BPAEOiqVigcL7DBwMzzWw5ocr2cOBGCvBa3f3d6PkDQnCfRj/57hZLIIgz3UWhSZ6+42zgL32Yl14R1R3/EnjF3X+Q9FZBXauZDY9KAphZNXAkoT3kMcJULFAA1wng7l9z91HuPprw//JRdz+TArtWM6sxs9rEa+BfgJfoJ9/dohlZbGbHEOoiE9NdXN/HWeo1ZnY7MJ0wpe1K4CrgbuAO4GPA28Cp7t65QTmvmNkhwJPAi2yuT76c0E5QMNdqZvsQGg5LCTdrd7j7NWa2G+GueSjwPPBZd2/qu5z2rqhq6BJ3P67QrjW6nruizTLgD+5+vZkNox98d4smEIiISGrFUjUkIiJdUCAQESlyCgQiIkVOgUBEpMgpEIiIFDkFApGImbVFM0MmHr02AZiZjU6eHVakP9EUEyKbbXL3SX2dCZFcU4lAJINoHvnvRXPJP2NmY6P9o83sUTN7wczmm9nHov07mtld0XoC/zCzg6JTlZrZL6I1Bh6KRg1jZl+K1lh4wczm9tFlShFTIBDZrLpT1dBpSe+tc/eJwE8II9QBfgz8xt33AW4Dbor23wT8NVpPYAqwJNo/DrjZ3fcC1gInRfsvAyZH5zkvWxcn0hWNLBaJmFmDuw9MsX85YaGYN6NJ71a4+zAz+xAY4e4t0f733X17M1sFjEqeEiGaNvvhaAESzOyrQLm7X2dmDwANhGlB7k5ai0AkJ1QiEInHu3jdHclz5bSxuY3uWMIKelOAhUmzborkhAKBSDynJT3/PXq9gDBjJsCZhAnxICw5eD50LDAzuKuTmlkJsIu7PwZ8FRgMbFUqEckm3XmIbFYdrQqW8IC7J7qQbmdmLxDu6mdF+74I/NrMvgKsAj4X7b8ImGNmnyfc+Z8PvE9qpcDvo2BhwE3RGgQiOaM2ApEMojaCqe7+YV/nRSQbVDUkIlLkVCIQESlyKhGIiBQ5BQIRkSKnQCAiUuQUCEREipwCgYhIkfv/nX+ZkEXWu6gAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_oi4PCrAkwD",
        "outputId": "a90ede95-c8da-42c3-ee33-515b7dac330a"
      },
      "source": [
        "train_mseVGG=history.history['mse']\n",
        "val_mseVGG=history.history['val_mse']\n",
        "print(\"MSE on Train set:\", train_mseVGG[53-1])\n",
        "print(\"MSE on Validation set:\", val_mseVGG[53-1])\n",
        "test_scoresVGG=vgg_model.evaluate(xTestN, yTestN, verbose=2)\n",
        "print(\"MSE on Test set:\", test_scoresVGG[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE on Train set: 0.03121822141110897\n",
            "MSE on Validation set: 0.03993906080722809\n",
            "20/20 - 0s - loss: 0.0399 - mse: 0.0399\n",
            "MSE on Test set: 0.03993600606918335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYBejNIajZTM"
      },
      "source": [
        "# Implementation analysis\n",
        "\n",
        "1. VGG16 base model was loaded keeping last two layers unfrozen. Addition of an extra convoluitonal layer before using Flatten helps in reducing the number of parameters of the model.Batch normalization works well as a regularization technique. Without the extra conv layer and batch normalisation, the mse was too high. So, vgg16 did not take more than 5 or 6 minutes to finish training. \n",
        "2.  Use of LeakyReLU instead of relu in the second last dense layer improved the model performance slightly.\n",
        "3. With adam as an optimzer the model's train mse was above 0.3.  Afterwards, RMSProp with a learning rate of 0.0001 was used which reduced the mse remarkably. The train and valid mse was declined to a stable state of 0.031 or 0.0377 after several consecutive training of the model with learning rate of 0.0001. The mse on the test dataset did  not also improve well.\n",
        "4.Although, it was expected that the mse will be decreased remarkably after using a vgg16 model. But it did not work since greyscale image pairs are used instead of RGB which again were stacked depthwise leaving the last channel blank. Vgg16 which is pretrained on imagenet does not seem to work on the greyscale image pairs. \n",
        "4. Rotation_range=3 and horizontal flip was used initially as data augmentation techniues but only horizontal flip worked well on the dataset improving the model performance slightly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZHi_lkZcmVJ"
      },
      "source": [
        "#Section 5: Siamese Network "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7WrudbpvXud"
      },
      "source": [
        "### Preparing rgb images with depth 3 for siamese model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj56dXuTw831",
        "outputId": "85f9c79c-271d-4962-b427-7b1f58d617fa"
      },
      "source": [
        "\n",
        "#Taking input images with depth 3 Need to rescale the values\n",
        "input1=xTrain[:,:,:,0:3]\n",
        "input2=xTrain[:,:,:,3:6]\n",
        "inputVal1=xValid[:,:,:,0:3]\n",
        "inputVal2=xValid[:,:,:,3:6]\n",
        "inputTest1=xTest[:,:,:,0:3]\n",
        "inputTest2=xTest[:,:,:,3:6]\n",
        "\n",
        "#rescaling the inputs image pixel\n",
        "input1=input1/255.0\n",
        "input2=input2/ 255.0\n",
        "inputVal1= inputVal1/255.0\n",
        "inputVal2=inputVal2 /255.0\n",
        "inputTest1=inputTest1/255.0\n",
        "inputTest2=inputTest2/255.0\n",
        "\n",
        "print(input1.shape[1:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(80, 50, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKjTBEFiwZrj"
      },
      "source": [
        "### Building New Feature Extractor Model vgg16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZetFVA1c44A",
        "outputId": "7a941189-b72c-4841-8099-d19196297d7b"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dropout\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "\n",
        "vgg16_model = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(80, 50, 3))\n",
        "\n",
        "for layer in vgg16_model.layers[:]:    \n",
        "    layer.trainable = False\n",
        "\n",
        "#model = vgg_model()\n",
        "model = vgg16_model\n",
        "\n",
        "# add new classifier layersflat1 = Flatten()model.get_layer('block4_pool').output (model.get_layer('block4_pool').output)\n",
        "flat1 = Flatten()(model.get_layer('block4_pool').output)\n",
        "class1 = Dense(512, activation = 'relu')(flat1)   \n",
        "batch1= BatchNormalization()(class1)\n",
        "output = Dense(1, activation='linear')(batch1)\n",
        "# define new model outputs=model.layers[-2].output\n",
        "vgg16model1 = Model(inputs=model.inputs,outputs=output )\n",
        "# summarize\n",
        "vgg16model1.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_65\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_36 (InputLayer)        [(None, 80, 50, 3)]       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 80, 50, 64)        1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 80, 50, 64)        36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 40, 25, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 40, 25, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 40, 25, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 20, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 20, 12, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 20, 12, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 20, 12, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 10, 6, 256)        0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 10, 6, 512)        1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 10, 6, 512)        2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 10, 6, 512)        2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 5, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_71 (Flatten)         (None, 7680)              0         \n",
            "_________________________________________________________________\n",
            "dense_140 (Dense)            (None, 512)               3932672   \n",
            "_________________________________________________________________\n",
            "batch_normalization_104 (Bat (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_141 (Dense)            (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 11,570,497\n",
            "Trainable params: 3,934,209\n",
            "Non-trainable params: 7,636,288\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1pR3q-5wpCM"
      },
      "source": [
        "### Simamese Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmb85a2tTVYZ",
        "outputId": "3851cff1-5c3c-433b-e1bf-9c4a37300aa5"
      },
      "source": [
        "#xTrainN.shape[1:]\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dropout, Activation\n",
        "from keras.layers import concatenate\n",
        "#initializing input size\n",
        "img_a_in = Input(shape = input1.shape[1:] , name = 'ImageA_Input')\n",
        "img_b_in = Input(shape = input1.shape[1:], name = 'ImageB_Input')\n",
        "\n",
        "#extracting featuers for two images from the vgg16model \n",
        "img_a_feat = vgg16model(img_a_in)\n",
        "img_b_feat = vgg16model(img_b_in)\n",
        "\n",
        "#concatenating thmze features and feeding it to the model\n",
        "merged_features = concatenate([img_a_feat, img_b_feat], name = 'merge_features')\n",
        "merged_features= Flatten()(merged_features)\n",
        "newLayer = Dense(32, activation = 'relu')(merged_features)\n",
        "newLayer = BatchNormalization()(newLayer)\n",
        "newLayer = Activation('relu')(newLayer)\n",
        "newLayer = Dense(16, activation = 'relu')(newLayer)\n",
        "newLayer = BatchNormalization()(newLayer)\n",
        "newLayer = Activation('relu')(newLayer)\n",
        "prediction = Dense(1, activation = 'linear', name='targets' )(newLayer)\n",
        "siamese_model = Model(inputs = [img_a_in, img_b_in], outputs = [prediction], name = 'Siamese_Model')\n",
        "siamese_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Siamese_Model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "ImageA_Input (InputLayer)       [(None, 80, 50, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "ImageB_Input (InputLayer)       [(None, 80, 50, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "functional_15 (Functional)      (None, 1)            15505729    ImageA_Input[0][0]               \n",
            "                                                                 ImageB_Input[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "merge_features (Concatenate)    (None, 2)            0           functional_15[34][0]             \n",
            "                                                                 functional_15[35][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_75 (Flatten)            (None, 2)            0           merge_features[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_148 (Dense)               (None, 32)           96          flatten_75[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 32)           128         dense_148[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 32)           0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_149 (Dense)               (None, 16)           528         activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 16)           64          dense_149[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 16)           0           batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "targets (Dense)                 (None, 1)            17          activation_37[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 15,506,562\n",
            "Trainable params: 7,869,154\n",
            "Non-trainable params: 7,637,408\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj-758jcIzGz"
      },
      "source": [
        "# use early stopping to optimally terminate training through callbacks\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=400)\n",
        "\n",
        "# save best model automatically\n",
        "mc= ModelCheckpoint('yourdirectory/simaese1.h5', monitor='val_loss', \n",
        "                    mode='min', verbose=1, save_best_only=True)\n",
        "cb_list=[es,mc]\n",
        "#lr=0.00005\n",
        "from keras import optimizers\n",
        "siamese_model.compile(optimizer=keras.optimizers.RMSprop(lr=0.000005),\n",
        "    loss=['mse'], \n",
        "    metrics=['mse'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcEN1H0t57le"
      },
      "source": [
        "### Using the Labels that were mapped in between (-1,1) in section 4 and stored in yTrainN, yValidN, yTestN again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lq4RYi3BuBc0",
        "outputId": "fccad7bb-2237-4038-b64d-e470b3ef500a"
      },
      "source": [
        "siamese_model.fit(\n",
        "        {\"ImageA_Input\": input1, \"ImageB_Input\": input2}, {\"targets\": yTrain},\n",
        "        epochs=400,\n",
        "        batch_size=32,\n",
        "        validation_data=({\"ImageA_Input\": inputVal1, \"ImageB_Input\": inputVal2}, {\"targets\": yValid}),\n",
        "    \n",
        "        callbacks=cb_list )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00001: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 35ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 29.4513 - val_mse: 29.4513\n",
            "Epoch 2/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00002: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 45.2087 - val_mse: 45.2087\n",
            "Epoch 3/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00003: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.1792 - val_mse: 0.1792\n",
            "Epoch 4/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00004: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.7384 - val_mse: 0.7384\n",
            "Epoch 5/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00005: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.1943 - val_mse: 0.1943\n",
            "Epoch 6/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00006: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 1.2305 - val_mse: 1.2305\n",
            "Epoch 7/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00007: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.7554 - val_mse: 0.7554\n",
            "Epoch 8/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00008: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 78.7010 - val_mse: 78.7010\n",
            "Epoch 9/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00009: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 1.6194 - val_mse: 1.6194\n",
            "Epoch 10/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00010: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 5.4777 - val_mse: 5.4777\n",
            "Epoch 11/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00011: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.4372 - val_mse: 0.4372\n",
            "Epoch 12/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00012: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 2.3237 - val_mse: 2.3237\n",
            "Epoch 13/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00013: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 2.0656 - val_mse: 2.0656\n",
            "Epoch 14/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00014: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.2026 - val_mse: 0.2026\n",
            "Epoch 15/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00015: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.3091 - val_mse: 0.3091\n",
            "Epoch 16/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00016: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 12.4978 - val_mse: 12.4978\n",
            "Epoch 17/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00017: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 9.2102 - val_mse: 9.2102\n",
            "Epoch 18/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00018: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0484 - val_mse: 0.0484\n",
            "Epoch 19/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00019: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.1083 - val_mse: 0.1083\n",
            "Epoch 20/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00020: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 15.2334 - val_mse: 15.2334\n",
            "Epoch 21/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00021: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 4.5689 - val_mse: 4.5689\n",
            "Epoch 22/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00022: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 3.6566 - val_mse: 3.6566\n",
            "Epoch 23/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00023: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 2.1220 - val_mse: 2.1220\n",
            "Epoch 24/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00024: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.4327 - val_mse: 0.4327\n",
            "Epoch 25/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00025: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.9662 - val_mse: 0.9662\n",
            "Epoch 26/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00026: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 2.4839 - val_mse: 2.4839\n",
            "Epoch 27/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00027: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.6989 - val_mse: 0.6989\n",
            "Epoch 28/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00028: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.1915 - val_mse: 0.1915\n",
            "Epoch 29/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00029: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 5.0295 - val_mse: 5.0295\n",
            "Epoch 30/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00030: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.5248 - val_mse: 0.5248\n",
            "Epoch 31/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00031: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.5807 - val_mse: 0.5807\n",
            "Epoch 32/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00032: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 1.6701 - val_mse: 1.6701\n",
            "Epoch 33/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00033: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.6171 - val_mse: 0.6171\n",
            "Epoch 34/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00034: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 12.9724 - val_mse: 12.9724\n",
            "Epoch 35/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00035: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.2180 - val_mse: 0.2180\n",
            "Epoch 36/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00036: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.0523 - val_mse: 0.0523\n",
            "Epoch 37/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00037: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.9532 - val_mse: 0.9532\n",
            "Epoch 38/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00038: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 1.7978 - val_mse: 1.7978\n",
            "Epoch 39/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00039: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 4.0992 - val_mse: 4.0992\n",
            "Epoch 40/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00040: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 48.2569 - val_mse: 48.2569\n",
            "Epoch 41/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00041: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.1371 - val_mse: 0.1371\n",
            "Epoch 42/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00042: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.3690 - val_mse: 0.3690\n",
            "Epoch 43/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00043: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.1004 - val_mse: 0.1004\n",
            "Epoch 44/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00044: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.5614 - val_mse: 0.5614\n",
            "Epoch 45/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00045: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 132.4322 - val_mse: 132.4322\n",
            "Epoch 46/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00046: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 12.4956 - val_mse: 12.4956\n",
            "Epoch 47/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00047: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.1487 - val_mse: 0.1487\n",
            "Epoch 48/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00048: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.0452 - val_mse: 0.0452\n",
            "Epoch 49/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00049: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 57.4652 - val_mse: 57.4652\n",
            "Epoch 50/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00050: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0465 - val_mse: 0.0465\n",
            "Epoch 51/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00051: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.1558 - val_mse: 0.1558\n",
            "Epoch 52/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00052: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.1190 - val_mse: 0.1190\n",
            "Epoch 53/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00053: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.3704 - val_mse: 0.3704\n",
            "Epoch 54/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00054: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.2397 - val_mse: 0.2397\n",
            "Epoch 55/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00055: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.3124 - val_mse: 0.3124\n",
            "Epoch 56/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00056: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.4131 - val_mse: 0.4131\n",
            "Epoch 57/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00057: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 38.8809 - val_mse: 38.8809\n",
            "Epoch 58/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00058: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.0335 - val_mse: 0.0335\n",
            "Epoch 59/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00059: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.4266 - val_mse: 0.4266\n",
            "Epoch 60/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00060: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.5426 - val_mse: 0.5426\n",
            "Epoch 61/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00061: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 8.1786 - val_mse: 8.1786\n",
            "Epoch 62/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00062: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.0347 - val_mse: 0.0347\n",
            "Epoch 63/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00063: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 4.0692 - val_mse: 4.0692\n",
            "Epoch 64/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00064: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.0700 - val_mse: 0.0700\n",
            "Epoch 65/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00065: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 7.7054 - val_mse: 7.7054\n",
            "Epoch 66/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00066: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.2528 - val_mse: 0.2528\n",
            "Epoch 67/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00067: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 5.8317 - val_mse: 5.8317\n",
            "Epoch 68/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00068: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 3.0415 - val_mse: 3.0415\n",
            "Epoch 69/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00069: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 93.8334 - val_mse: 93.8334\n",
            "Epoch 70/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00070: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 23.6007 - val_mse: 23.6007\n",
            "Epoch 71/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00071: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.3278 - val_mse: 0.3278\n",
            "Epoch 72/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00072: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 16.5487 - val_mse: 16.5487\n",
            "Epoch 73/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00073: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 1.3841 - val_mse: 1.3841\n",
            "Epoch 74/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00074: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 17.0961 - val_mse: 17.0961\n",
            "Epoch 75/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00075: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.9071 - val_mse: 0.9071\n",
            "Epoch 76/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00076: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0590 - val_mse: 0.0590\n",
            "Epoch 77/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00077: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 1.4510 - val_mse: 1.4510\n",
            "Epoch 78/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00078: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.2003 - val_mse: 0.2003\n",
            "Epoch 79/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00079: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 2.7075 - val_mse: 2.7075\n",
            "Epoch 80/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00080: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.2197 - val_mse: 0.2197\n",
            "Epoch 81/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00081: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.5519 - val_mse: 0.5519\n",
            "Epoch 82/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00082: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 3.8569 - val_mse: 3.8569\n",
            "Epoch 83/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00083: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.2056 - val_mse: 0.2056\n",
            "Epoch 84/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00084: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.3900 - val_mse: 0.3900\n",
            "Epoch 85/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00085: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0882 - val_mse: 0.0882\n",
            "Epoch 86/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00086: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 3.0915 - val_mse: 3.0915\n",
            "Epoch 87/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00087: val_loss did not improve from 0.03222\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.1861 - val_mse: 0.1861\n",
            "Epoch 88/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00088: val_loss improved from 0.03222 to 0.03205, saving model to yourdirectory/simaese1.h5\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0321 - val_mse: 0.0321\n",
            "Epoch 89/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00089: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 31.9611 - val_mse: 31.9611\n",
            "Epoch 90/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00090: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 2.9121 - val_mse: 2.9121\n",
            "Epoch 91/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00091: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.2943 - val_mse: 0.2943\n",
            "Epoch 92/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00092: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.4704 - val_mse: 0.4704\n",
            "Epoch 93/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00093: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 2.9114 - val_mse: 2.9114\n",
            "Epoch 94/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00094: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0930 - val_mse: 0.0930\n",
            "Epoch 95/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00095: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 1.1699 - val_mse: 1.1699\n",
            "Epoch 96/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00096: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.4712 - val_mse: 0.4712\n",
            "Epoch 97/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00097: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0653 - val_mse: 0.0653\n",
            "Epoch 98/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00098: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 14.7267 - val_mse: 14.7267\n",
            "Epoch 99/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00099: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 2.8402 - val_mse: 2.8402\n",
            "Epoch 100/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00100: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0543 - val_mse: 0.0543\n",
            "Epoch 101/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00101: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 11.1994 - val_mse: 11.1994\n",
            "Epoch 102/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00102: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 24.2740 - val_mse: 24.2740\n",
            "Epoch 103/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00103: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.5325 - val_mse: 0.5325\n",
            "Epoch 104/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00104: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 63.7306 - val_mse: 63.7306\n",
            "Epoch 105/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00105: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.0341 - val_mse: 0.0341\n",
            "Epoch 106/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00106: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 17.9520 - val_mse: 17.9520\n",
            "Epoch 107/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00107: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.0792 - val_mse: 0.0792\n",
            "Epoch 108/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00108: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.2116 - val_mse: 0.2116\n",
            "Epoch 109/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00109: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0595 - val_mse: 0.0595\n",
            "Epoch 110/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00110: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0409 - val_mse: 0.0409\n",
            "Epoch 111/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00111: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 14.1971 - val_mse: 14.1971\n",
            "Epoch 112/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00112: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 1.3637 - val_mse: 1.3637\n",
            "Epoch 113/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00113: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.2905 - val_mse: 0.2905\n",
            "Epoch 114/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00114: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 46.0849 - val_mse: 46.0849\n",
            "Epoch 115/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00115: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.1670 - val_mse: 0.1670\n",
            "Epoch 116/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00116: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0331 - val_mse: 0.0331\n",
            "Epoch 117/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00117: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.3245 - val_mse: 0.3245\n",
            "Epoch 118/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00118: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.3433 - val_mse: 0.3433\n",
            "Epoch 119/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00119: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0448 - val_mse: 0.0448\n",
            "Epoch 120/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00120: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 3.6347 - val_mse: 3.6347\n",
            "Epoch 121/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00121: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 3.9716 - val_mse: 3.9716\n",
            "Epoch 122/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00122: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0354 - val_mse: 0.0354\n",
            "Epoch 123/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00123: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0335 - val_mse: 0.0335\n",
            "Epoch 124/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00124: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 45.9915 - val_mse: 45.9915\n",
            "Epoch 125/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00125: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.0329 - val_mse: 0.0329\n",
            "Epoch 126/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0313 - mse: 0.0313\n",
            "Epoch 00126: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0313 - mse: 0.0313 - val_loss: 0.6716 - val_mse: 0.6716\n",
            "Epoch 127/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00127: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.5160 - val_mse: 0.5160\n",
            "Epoch 128/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00128: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 4.3876 - val_mse: 4.3876\n",
            "Epoch 129/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00129: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.0802 - val_mse: 0.0802\n",
            "Epoch 130/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00130: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.3943 - val_mse: 0.3943\n",
            "Epoch 131/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00131: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 1.6111 - val_mse: 1.6111\n",
            "Epoch 132/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00132: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 34ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 2.8041 - val_mse: 2.8041\n",
            "Epoch 133/400\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.0312 - mse: 0.0312\n",
            "Epoch 00133: val_loss did not improve from 0.03205\n",
            "88/88 [==============================] - 3s 33ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 1.9987 - val_mse: 1.9987\n",
            "Epoch 134/400\n",
            "13/88 [===>..........................] - ETA: 2s - loss: 0.0312 - mse: 0.0312"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-126-35165fc9fa80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"ImageA_Input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputVal1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ImageB_Input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputVal2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myValid\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         callbacks=cb_list )\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "Hfc9_OGry9KR",
        "outputId": "c9e9459b-8392-47f8-acfa-04055b04b49b"
      },
      "source": [
        "# plot training and validation accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['mse'])\n",
        "plt.plot(history.history['val_mse'])\n",
        "plt.ylim([0,0.6])\n",
        "plt.ylabel('Mean Squarred Error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.savefig(\"SiameseModel2.png\", dpi=300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X2Wqy__yzHj"
      },
      "source": [
        "train_mseSimaese=history.history['mse']\n",
        "val_mseSimaese=history.history['val_mse']\n",
        "test_scoresSimaese=regressor_model.evaluate(xTest, yTest, verbose=2)\n",
        "print(\"MSE on Train set:\", train_mseSimaese[153-1])\n",
        "print(\"MSE on Validation set:\", val_mseSimaese[153-1])\n",
        "print(\"MSE on Test set::\", test_scoresSimaese[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeD5VzZJVG1J"
      },
      "source": [
        "## Analysis:\n",
        "1. I have tried to load the model in seciton 5 but the newly loading vgg base network makes it convenient to build a feature extarctor.\n",
        "2. The second last fully connected layer is used as the output layer in the vgg16 feature extractor model. A flatten layer is added after the last pooling layer of vgg16.\n",
        "3. For 'tanh' as an activation function, train mse was 0.06 while val_mse was 12.4 which is unexpected. However, on the siamese model 'sigmoid' as the activation function works well.\n",
        "4. with lr=0.0001 the mse was 0.032 val_mse=0.0319\n"
      ]
    }
  ]
}